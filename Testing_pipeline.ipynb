{"cells":[{"cell_type":"markdown","metadata":{"id":"3X1_OWD1DM6j"},"source":["#Set Directories"]},{"cell_type":"code","execution_count":166,"metadata":{"executionInfo":{"elapsed":527,"status":"ok","timestamp":1733423923531,"user":{"displayName":"Thomas Garity","userId":"11772081689971931307"},"user_tz":300},"id":"isnxF4KeC-_X"},"outputs":[],"source":["import os\n","\n","# Directories - change based on where your project is saved\n","project_dir = \"/content/drive/MyDrive/CS2822_Final_Project\"\n","\n","# CHANGE THESE\n","normal_model_name = \"entire_dataset_resnet18.a1-e=12-lr=0.01_limit=20.pt\"\n","bugged_model_name = \"o2o_easy_resnet18.a1-e=2-lr=0.01_limit=20.pt\"\n","concept_instance_name = \"CRAFT_Corgi_Snow_320_im_8_concepts_o2o_easy_resnet18.a1-e=12-lr=0.01_limit=20.pt\""]},{"cell_type":"code","execution_count":101,"metadata":{"executionInfo":{"elapsed":523,"status":"ok","timestamp":1733420849861,"user":{"displayName":"Thomas Garity","userId":"11772081689971931307"},"user_tz":300},"id":"9FhfWKiuDIi0"},"outputs":[],"source":["models_dir = os.path.join(project_dir, \"Models\")\n","datasets_dir = os.path.join(project_dir, \"Datasets\")\n","RESULTS_DIR = os.path.join(project_dir, \"Results\")\n","EXPERIMENT_OUTPUT_DIR = os.path.join(project_dir, \"Final_Results\")\n","MODEL_NAME = \"resnet18.a1_in1k\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16478,"status":"ok","timestamp":1733417206759,"user":{"displayName":"Thomas Garity","userId":"11772081689971931307"},"user_tz":300},"id":"JeDLq4wzDMhf","outputId":"1b2f62a9-f10e-46e5-975a-5392a3060bda"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["# Mount google drive to local dir\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"]},{"cell_type":"markdown","metadata":{"id":"5z0eDI3rDPmm"},"source":["# Dependencies"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dEe2jfjeDSuM"},"outputs":[],"source":["import argparse\n","\n","import torch\n","import torch.optim as optim\n","from torch import nn\n","from torch.nn import Module\n","from torch.optim import Optimizer\n","from torch.utils.data import DataLoader\n","from torchvision import models\n","from tqdm import tqdm\n","from tqdm.auto import tqdm\n","import timm\n","import wandb\n","import os\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.decomposition import NMF\n","from einops import rearrange\n","from scipy.optimize import minimize\n","import pandas as pd\n"]},{"cell_type":"markdown","metadata":{"id":"axvRnmqoDU2B"},"source":["#0. Spawrious Source Code"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c9WgMW9hDYBB"},"outputs":[],"source":["import os\n","import tarfile\n","import urllib\n","import urllib.request\n","from typing import Any, Tuple\n","\n","import torch\n","from PIL import Image\n","from torch.utils.data import ConcatDataset, Dataset\n","from torchvision import transforms\n","from torchvision.datasets import ImageFolder\n","from tqdm import tqdm\n","import timm\n","from PIL import ImageFile\n","\n","ImageFile.LOAD_TRUNCATED_IMAGES = True\n","\n","# MODEL_NAME = \"vit_so400m_patch14_siglip_384\"\n","# MODEL_NAME = 'swin_base_patch4_window7_224.ms_in22k_ft_in1k'\n","# MODEL_NAME = 'deit3_base_patch16_224.fb_in22k_ft_in1k'\n","# MODEL_NAME = 'beit_base_patch16_224.in22k_ft_in22k_in1k'\n","# MODEL_NAME = 'eva02_base_patch14_448.mim_in22k_ft_in22k_in1k'\n","# MODEL_NAME = 'levit_128s.fb_dist_in1k'\n","\n","def set_model_name(name):\n","    global MODEL_NAME\n","    MODEL_NAME = name\n","\n","\n","def _extract_dataset_from_tar(\n","    tar_file_name: str, data_dir: str\n",") -> None:\n","    tar_file_dst = os.path.join(data_dir, tar_file_name)\n","    print(\"Extracting dataset...\")\n","    tar = tarfile.open(tar_file_dst, \"r:gz\")\n","    tar.extractall(os.path.dirname(tar_file_dst))\n","    tar.close()\n","\n","\n","def _download_dataset_if_not_available(\n","    dataset_name: str, data_dir: str, remove_tar_after_extracting: bool = True\n",") -> None:\n","    \"\"\"\n","    datasets.txt file, which is present in the data_dir, is used to check if the dataset is already extracted. If the dataset is already extracted, then the tar file is not downloaded again.\n","    \"\"\"\n","    data_dir = data_dir.split(\"/spawrious224/\")[\n","        0\n","    ]  # in case people pass in the wrong root_dir\n","    os.makedirs(data_dir, exist_ok=True)\n","    dataset_name = dataset_name.lower()\n","    if dataset_name.split(\"_\")[0] == \"m2m\":\n","        dataset_name = \"entire_dataset\"\n","    url_dict = {\n","        \"entire_dataset\": \"https://www.dropbox.com/s/hofkueo8qvaqlp3/spawrious224__entire_dataset.tar.gz?dl=1\",\n","        \"o2o_easy\": \"https://www.dropbox.com/s/kwhiv60ihxe3owy/spawrious224__o2o_easy.tar.gz?dl=1\",\n","        \"o2o_medium\": \"https://www.dropbox.com/s/x03gkhdwar5kht4/spawrious224__o2o_medium.tar.gz?dl=1\",\n","        \"o2o_hard\": \"https://www.dropbox.com/s/p1ry121m2gjj158/spawrious224__o2o_hard.tar.gz?dl=1\",\n","        # \"m2m\": \"https://www.dropbox.com/s/5usem63nfub266y/spawrious__m2m.tar.gz?dl=1\",\n","    }\n","    tar_file_name = f\"spawrious224__{dataset_name}.tar.gz\"\n","    tar_file_dst = os.path.join(data_dir, tar_file_name)\n","    url = url_dict[dataset_name]\n","\n","    # check if the dataset is already extracted\n","    if _check_images_availability(data_dir, dataset_name):\n","        print(\"Dataset already downloaded and extracted.\")\n","        return\n","    # check if the tar file is already downloaded\n","    else:\n","        if os.path.exists(tar_file_dst):\n","            print(\"Dataset already downloaded. Extracting...\")\n","            _extract_dataset_from_tar(\n","                tar_file_name, data_dir\n","            )\n","            return\n","        # download the tar file and extract from it\n","        else:\n","            print(\"Dataset not found. Downloading...\")\n","            response = urllib.request.urlopen(url)\n","            total_size = int(response.headers.get(\"Content-Length\", 0))\n","            block_size = 1024\n","            # Track progress of download\n","            progress_bar = tqdm(total=total_size, unit=\"iB\", unit_scale=True)\n","            with open(tar_file_dst, \"wb\") as f:\n","                while True:\n","                    buffer = response.read(block_size)\n","                    if not buffer:\n","                        break\n","                    f.write(buffer)\n","                    progress_bar.update(len(buffer))\n","            progress_bar.close()\n","            print(\"Dataset downloaded. Extracting...\")\n","            _extract_dataset_from_tar(\n","                tar_file_name, data_dir\n","            )\n","            return\n","\n","\n","class CustomImageFolder(Dataset):\n","    \"\"\"\n","    A class that takes one folder at a time and loads a set number of images in a folder and assigns them a specific class\n","    \"\"\"\n","\n","    def __init__(\n","        self, folder_path, class_index, location_index, limit=None, transform=None\n","    ):\n","        self.folder_path = folder_path\n","        self.class_index = class_index\n","        self.location_index = location_index\n","        self.image_paths = [\n","            os.path.join(folder_path, img)\n","            for img in os.listdir(folder_path)\n","            if img.endswith((\".png\", \".jpg\", \".jpeg\"))\n","        ]\n","        if limit:\n","            self.image_paths = self.image_paths[:limit]\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.image_paths)\n","\n","    def __getitem__(self, index: int) -> Tuple[Any, Any, Any]:\n","        img_path = self.image_paths[index]\n","        img = Image.open(img_path).convert(\"RGB\")\n","\n","        if self.transform:\n","            img = self.transform(img)\n","\n","        class_label = torch.tensor(self.class_index, dtype=torch.long)\n","        location_label = torch.tensor(self.location_index, dtype=torch.long)\n","        return img, class_label, location_label\n","\n","\n","class MultipleDomainDataset:\n","    N_STEPS = 5001  # Default, subclasses may override\n","    CHECKPOINT_FREQ = 100  # Default, subclasses may override\n","    N_WORKERS = 8  # Default, subclasses may override\n","    ENVIRONMENTS = None  # Subclasses should override\n","    INPUT_SHAPE = None  # Subclasses should override\n","\n","    def __getitem__(self, index):\n","        return self.datasets[index]\n","\n","    def __len__(self):\n","        return len(self.datasets)\n","\n","\n","def build_combination(benchmark_type, group, test, filler=None):\n","    total = 3168\n","    combinations = {}\n","    if \"m2m\" in benchmark_type:\n","        counts = [total, total]\n","        combinations[\"train_combinations\"] = {\n","            (\"bulldog\",): [(group[0], counts[0]), (group[1], counts[1])],\n","            (\"dachshund\",): [(group[1], counts[0]), (group[0], counts[1])],\n","            (\"labrador\",): [(group[2], counts[0]), (group[3], counts[1])],\n","            (\"corgi\",): [(group[3], counts[0]), (group[2], counts[1])],\n","        }\n","        combinations[\"test_combinations\"] = {\n","            (\"bulldog\",): [test[0], test[1]],\n","            (\"dachshund\",): [test[1], test[0]],\n","            (\"labrador\",): [test[2], test[3]],\n","            (\"corgi\",): [test[3], test[2]],\n","        }\n","    else:\n","        counts = [int(0.97 * total), int(0.87 * total)]\n","        combinations[\"train_combinations\"] = {\n","            (\"bulldog\",): [(group[0], counts[0]), (group[0], counts[1])],\n","            (\"dachshund\",): [(group[1], counts[0]), (group[1], counts[1])],\n","            (\"labrador\",): [(group[2], counts[0]), (group[2], counts[1])],\n","            (\"corgi\",): [(group[3], counts[0]), (group[3], counts[1])],\n","            (\"bulldog\", \"dachshund\", \"labrador\", \"corgi\"): [\n","                (filler, total - counts[0]),\n","                (filler, total - counts[1]),\n","            ],\n","        }\n","        combinations[\"test_combinations\"] = {\n","            (\"bulldog\",): [test[0], test[0]],\n","            (\"dachshund\",): [test[1], test[1]],\n","            (\"labrador\",): [test[2], test[2]],\n","            (\"corgi\",): [test[3], test[3]],\n","        }\n","    return combinations\n","\n","\n","def _get_combinations(benchmark_type: str) -> Tuple[dict, dict]:\n","    combinations = {\n","        \"o2o_easy\": (\n","            [\"desert\", \"jungle\", \"dirt\", \"snow\"],\n","            [\"dirt\", \"snow\", \"desert\", \"jungle\"],\n","            \"beach\",\n","        ),\n","        \"o2o_medium\": (\n","            [\"mountain\", \"beach\", \"dirt\", \"jungle\"],\n","            [\"jungle\", \"dirt\", \"beach\", \"snow\"],\n","            \"desert\",\n","        ),\n","        \"o2o_hard\": (\n","            [\"jungle\", \"mountain\", \"snow\", \"desert\"],\n","            [\"mountain\", \"snow\", \"desert\", \"jungle\"],\n","            \"beach\",\n","        ),\n","        \"m2m_hard\": (\n","            [\"dirt\", \"jungle\", \"snow\", \"beach\"],\n","            [\"snow\", \"beach\", \"dirt\", \"jungle\"],\n","            None,\n","        ),\n","        \"m2m_easy\": (\n","            [\"desert\", \"mountain\", \"dirt\", \"jungle\"],\n","            [\"dirt\", \"jungle\", \"mountain\", \"desert\"],\n","            None,\n","        ),\n","        \"m2m_medium\": (\n","            [\"beach\", \"snow\", \"mountain\", \"desert\"],\n","            [\"desert\", \"mountain\", \"beach\", \"snow\"],\n","            None,\n","        ),\n","    }\n","    if benchmark_type not in combinations:\n","        raise ValueError(\"Invalid benchmark type\")\n","    group, test, filler = combinations[benchmark_type]\n","    return build_combination(benchmark_type, group, test, filler)\n","\n","\n","class SpawriousBenchmark(MultipleDomainDataset):\n","    ENVIRONMENTS = [\"Test\", \"SC_group_1\", \"SC_group_2\"]\n","    input_shape = (3, 224, 224)\n","    num_classes = 4\n","    class_list = [\"bulldog\", \"corgi\", \"dachshund\", \"labrador\"]\n","    locations_list = [\"desert\", \"jungle\", \"dirt\", \"mountain\", \"snow\", \"beach\"]\n","\n","    def __init__(self, benchmark, root_dir, augment=True):\n","        combinations = _get_combinations(benchmark.lower())\n","        self.type1 = benchmark.lower().startswith(\"o2o\")\n","        train_datasets, test_datasets = self._prepare_data_lists(\n","            combinations[\"train_combinations\"],\n","            combinations[\"test_combinations\"],\n","            root_dir,\n","            augment,\n","        )\n","        self.datasets = [ConcatDataset(test_datasets)] + train_datasets\n","\n","    def get_train_dataset(self):\n","        return torch.utils.data.ConcatDataset(self.datasets[1:])\n","\n","    def get_test_dataset(self):\n","        return self.datasets[0]\n","\n","    # Prepares the train and test data lists by applying the necessary transformations.\n","    def _prepare_data_lists(\n","        self, train_combinations, test_combinations, root_dir, augment\n","    ):\n","        backbone = timm.create_model(\n","            # \"vit_so400m_patch14_siglip_384\",\n","            MODEL_NAME,\n","            pretrained=True,\n","            num_classes=0,\n","        ).eval()\n","        self.data_config = timm.data.resolve_model_data_config(backbone)\n","        test_transforms = timm.data.create_transform(\n","            **self.data_config, is_training=False\n","        )\n","\n","        # test_transforms = transforms.Compose(\n","        #     [\n","        #         transforms.Resize((self.input_shape[1], self.input_shape[2])),\n","        #         transforms.transforms.ToTensor(),\n","        #         transforms.Normalize(\n","        #             mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n","        #         ),\n","        #     ]\n","        # )\n","\n","        if augment:\n","            train_transforms = timm.data.create_transform(\n","                **self.data_config, is_training=True\n","            )\n","        else:\n","            train_transforms = test_transforms\n","        print(\"Creating Training Dataset:\")\n","        train_data_list = self._create_data_list(\n","            train_combinations, root_dir, train_transforms\n","        )\n","        print(\"Creating Testing Dataset:\")\n","        test_data_list = self._create_data_list(\n","            test_combinations, root_dir, test_transforms\n","        )\n","\n","        return train_data_list, test_data_list\n","\n","    # Creates a list of datasets based on the given combinations and transformations.\n","    def _create_data_list(self, combinations, root_dir, transforms):\n","        data_list = []\n","        if isinstance(combinations, dict):\n","            # Build class groups for a given set of combinations, root directory, and transformations.\n","            for_each_class_group = []\n","            cg_index = 0\n","            for classes, comb_list in combinations.items():\n","                for_each_class_group.append([])\n","                for ind, location_limit in enumerate(comb_list):\n","                    if isinstance(location_limit, tuple):\n","                        location, limit = location_limit\n","                    else:\n","                        location, limit = location_limit, None\n","                    cg_data_list = []\n","                    for cls in classes:\n","                        path = os.path.join(\n","                            root_dir,\n","                            \"spawrious224\",\n","                            f\"{0 if not self.type1 else ind}/{location}/{cls}\",\n","                        )\n","                        print(f\"    Combination: {0 if not self.type1 else ind}/{location}/{cls}\")\n","                        print(f\"    Limit: {limit}\")\n","                        data = CustomImageFolder(\n","                            folder_path=path,\n","                            class_index=self.class_list.index(cls),\n","                            location_index=self.locations_list.index(location),\n","                            limit=limit,\n","                            transform=transforms,\n","                        )\n","                        cg_data_list.append(data)\n","\n","                    for_each_class_group[cg_index].append(ConcatDataset(cg_data_list))\n","                cg_index += 1\n","\n","            for group in range(len(for_each_class_group[0])):\n","                data_list.append(\n","                    ConcatDataset(\n","                        [\n","                            for_each_class_group[k][group]\n","                            for k in range(len(for_each_class_group))\n","                        ]\n","                    )\n","                )\n","        else:\n","            for location in combinations:\n","                path = os.path.join(root_dir, f\"{0}/{location}/\")\n","                data = ImageFolder(root=path, transform=transforms)\n","                data_list.append(data)\n","\n","        return data_list\n","\n","\n","def _check_images_availability(root_dir: str, dataset_type: str) -> bool:\n","    # Get the combinations for the given dataset type\n","    root_dir = root_dir.split(\"/spawrious224/\")[\n","        0\n","    ]  # in case people pass in the wrong root_dir\n","    if dataset_type == \"entire_dataset\":\n","        for dataset in [\"0\", \"1\", \"domain_adaptation_ds\"]:\n","            for location in [\"snow\", \"jungle\", \"desert\", \"dirt\", \"mountain\", \"beach\"]:\n","                for cls in [\"bulldog\", \"corgi\", \"dachshund\", \"labrador\"]:\n","                    path = os.path.join(\n","                        root_dir, \"spawrious224\", f\"{dataset}/{location}/{cls}\"\n","                    )\n","                    if not os.path.exists(path) or not any(\n","                        img.endswith((\".png\", \".jpg\", \".jpeg\"))\n","                        for img in os.listdir(path)\n","                    ):\n","                        return False\n","        return True\n","    combinations = _get_combinations(dataset_type.lower())\n","\n","    # Extract the train and test combinations\n","    train_combinations = combinations[\"train_combinations\"]\n","    test_combinations = combinations[\"test_combinations\"]\n","\n","    # Check if the relevant images for each combination are present in the root directory\n","    for combination in [train_combinations, test_combinations]:\n","        for classes, comb_list in combination.items():\n","            for ind, location_limit in enumerate(comb_list):\n","                if isinstance(location_limit, tuple):\n","                    location, limit = location_limit\n","                else:\n","                    location, limit = location_limit, None\n","\n","                for cls in classes:\n","                    path = os.path.join(\n","                        root_dir,\n","                        \"spawrious224\",\n","                        f\"{0 if not dataset_type.lower().startswith('o2o') else ind}/{location}/{cls}\",\n","                    )\n","\n","                    # If the path does not exist or there are no relevant images, return False\n","                    if not os.path.exists(path) or not any(\n","                        img.endswith((\".png\", \".jpg\", \".jpeg\"))\n","                        for img in os.listdir(path)\n","                    ):\n","                        return False\n","\n","    # If all the required images are present, return True\n","    return True\n","\n","\n","def get_spawrious_dataset(root_dir: str, dataset_name: str = \"entire_dataset\"):\n","    \"\"\"\n","    Returns the dataset as a torch dataset, and downloads dataset if dataset is not already available.\n","\n","    By default, the entire dataset is downloaded, which is necessary for m2m experiments, and domain adaptation experiments\n","    \"\"\"\n","    root_dir = root_dir.split(\"/spawrious224/\")[\n","        0\n","    ]  # in case people pass in the wrong root_dir\n","    assert dataset_name.lower() in {\n","        \"o2o_easy\",\n","        \"o2o_medium\",\n","        \"o2o_hard\",\n","        \"m2m_easy\",\n","        \"m2m_medium\",\n","        \"m2m_hard\",\n","        \"m2m\",\n","        \"entire_dataset\",\n","    }, f\"Invalid dataset type: {dataset_name}\"\n","    _download_dataset_if_not_available(dataset_name, root_dir)\n","    # TODO: get m2m to use entire dataset, not half of it\n","    return SpawriousBenchmark(dataset_name, root_dir, augment=True)"]},{"cell_type":"markdown","metadata":{"id":"N_4OpEfc9bd8"},"source":["#1. Load Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3804,"status":"ok","timestamp":1733417227753,"user":{"displayName":"Thomas Garity","userId":"11772081689971931307"},"user_tz":300},"id":"19CGVT0b99eP","outputId":"8da9af80-6998-424d-f3ed-b9d3f590f345"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["# Mount google drive to local dir\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zNLsJ13zBPZK"},"outputs":[],"source":["# Create data dir\n","!mkdir /content/data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cy0rwjcz_Ae2"},"outputs":[],"source":["# Set Model Name (using global var)\n","set_model_name(MODEL_NAME)\n","\n","import tarfile\n","\n","def download_dataset(dataset_name):\n","  # Unzip the  dataset\n","  drive_dir = os.path.join(datasets_dir, dataset_name)\n","  !cp $drive_dir /content/data\n","  tar_file = os.path.join('/content/data', drive_dir.split('/')[-1])\n","  data_dir = \"/content/data/\"\n","\n","  # Extract the dataset\n","  with tarfile.open(tar_file, 'r:gz') as tar:  # Use 'r' for .tar and 'r:gz' for .tar.gz\n","      tar.extractall(path=data_dir)\n","\n","# Download the datasets\n","download_dataset(\"spawrious224__o2o_easy.tar.gz\")\n","download_dataset(\"background_only_dataset.tar.gz\")"]},{"cell_type":"markdown","metadata":{"id":"rK07iVJDsFi1"},"source":["# 2. Download both Models\n","models are saved to variables 'normal_model' and 'bugged_model'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u9No4wq3tG5d"},"outputs":[],"source":["def load_model(model_name, model_type=\"resnet18.a1_in1k\"):\n","\n","    checkpoint_path = os.path.join(models_dir, model_name)\n","\n","    if model_type != \"resnet18.a1_in1k\":\n","        print(\"Only resnet18 is supported currently\")\n","        return\n","\n","    # Checks if GPU is available first\n","    if torch.cuda.is_available():\n","        model_weights = torch.load(os.path.join(models_dir, checkpoint_path))\n","    else:\n","        model_weights = torch.load(os.path.join(models_dir, checkpoint_path), map_location=torch.device('cpu'))\n","\n","    # Create model backbone (base model)\n","    model = timm.create_model(\n","                model_type,\n","                pretrained=True,\n","                num_classes=4).eval()\n","\n","    # Load weights\n","    model.load_state_dict(model_weights)\n","\n","    # set ENTIRE model to eval mode\n","    model.eval()\n","\n","    # Switch weights to cuda if possible\n","    if torch.cuda.is_available():\n","        model.to('cuda')\n","\n","    return model\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","referenced_widgets":["5dd888647003455391466a1f7aaaa37a","4616a8ae8004437596ac51878db06f1c","75e55fecf8aa48e88b603a0c6230ab84","af2eb4c3d9de4160b3992638bad61afc","f9af26d8401041598548e3f064a802dc","d6ffc141854a41a79e82d68cadfd0e7e","5d0ef6b7cb934376a45b9edc9de228a1","60c15e34fa9542f0918df03baa91ee49","3717b759ff23449dadb62c60d3fe4f8d","f4bfe880b05049fc87131ee6c7b7e55e","ead64455521c4522852cd087aa7a967f"]},"executionInfo":{"elapsed":13956,"status":"ok","timestamp":1733417466350,"user":{"displayName":"Thomas Garity","userId":"11772081689971931307"},"user_tz":300},"id":"vN54SIUysoxK","outputId":"0d82876b-34a1-4cf1-f299-cc78bc404c91"},"outputs":[{"name":"stderr","output_type":"stream","text":["<ipython-input-9-a9afaae834bd>:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model_weights = torch.load(os.path.join(models_dir, checkpoint_path))\n","/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5dd888647003455391466a1f7aaaa37a","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/46.8M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["normal_model = load_model(normal_model_name, \"resnet18.a1_in1k\")\n","bugged_model = load_model(bugged_model_name, \"resnet18.a1_in1k\")"]},{"cell_type":"markdown","metadata":{"id":"0wXhGAsM2UGj"},"source":["#3. Concept Extraction with Nonnegative Matrix Factorization"]},{"cell_type":"markdown","metadata":{"id":"lJ2k8Znd8lHd"},"source":["## Helpers"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":536,"status":"ok","timestamp":1733417823521,"user":{"displayName":"Thomas Garity","userId":"11772081689971931307"},"user_tz":300},"id":"ap_0yTLx8k3p","outputId":"09a31496-9a51-4bd8-d21d-b90ec76fa800"},"outputs":[{"name":"stderr","output_type":"stream","text":["<ipython-input-38-a9b8f2bf6f2f>:83: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.\n","  base_cmap = plt.cm.get_cmap(color_input)\n"]}],"source":["import cv2\n","from skimage import measure\n","import matplotlib.colors as mcolors\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","\n","def set_size(w,h):\n","  \"\"\"Set matplot figure size\"\"\"\n","  plt.rcParams[\"figure.figsize\"] = [w,h]\n","\n","def show(img, norm=True, **kwargs):\n","  \"\"\" Display torch/tf tensor \"\"\"\n","  if isinstance(img, torch.Tensor):\n","    img = img.detach().cpu().numpy()\n","  img = np.array(img, dtype=np.float32)\n","\n","  # check if channel first\n","  if img.shape[0] == 1:\n","    img = img[0]\n","  if img.shape[0] == 3:\n","    img = np.moveaxis(img, 0, 2)\n","  # check if cmap\n","  if img.shape[-1] == 1:\n","    img = img[:,:,0]\n","  # normalize\n","  if norm:\n","    img -= img.min(); img/=img.max()\n","\n","  plt.imshow(img, **kwargs)\n","  plt.axis('off')\n","  plt.grid(None)\n","\n","\n","all_cmaps = []\n","\n","def get_alpha_cmap(rgb_color):\n","    # Normalize the RGB color values to the range [0, 1]\n","    rgb_color = np.array(rgb_color) / 255.0\n","\n","    # Create an alpha colormap with varying alpha values\n","    cmap_data = {'red':   [(0, rgb_color[0], rgb_color[0]),\n","                           (1, rgb_color[0], rgb_color[0])],\n","                 'green': [(0, rgb_color[1], rgb_color[1]),\n","                           (1, rgb_color[1], rgb_color[1])],\n","                 'blue':  [(0, rgb_color[2], rgb_color[2]),\n","                           (1, rgb_color[2], rgb_color[2])],\n","                 'alpha': [(0, 0, 0),\n","                           (1, 1, 1)]}\n","\n","    alpha_cmap = mcolors.LinearSegmentedColormap('alpha_cmap', cmap_data, N=256)\n","\n","    all_cmaps.append(alpha_cmap)\n","\n","    return alpha_cmap\n","\n","slack_blue  = get_alpha_cmap((54, 197, 240))\n","slack_green = get_alpha_cmap((46, 182, 125) )\n","slack_red   = get_alpha_cmap((210, 40, 95))\n","slack_yellow = get_alpha_cmap((236, 178, 46))\n","slack_violet = get_alpha_cmap((84, 25, 85))\n","\n","google_blue = get_alpha_cmap((66, 133, 244))\n","google_red = get_alpha_cmap((219, 68, 55))\n","google_yellow= get_alpha_cmap((244, 180, 0))\n","google_green = get_alpha_cmap((15, 157, 88))\n","\n","deep_purple = get_alpha_cmap((103, 58, 183))\n","pink = get_alpha_cmap((236, 64, 122))\n","anthracite = get_alpha_cmap((13, 13, 21))\n","\n","palette_1 = get_alpha_cmap((33, 115, 174))\n","palette_2 = get_alpha_cmap((216, 146, 49))\n","palette_3 = get_alpha_cmap((58, 155, 118))\n","palette_4 = get_alpha_cmap((210, 158, 201))\n","\n","cmaps = [google_blue, slack_green, slack_red, slack_yellow, slack_violet, anthracite, google_blue, pink, deep_purple]\n","\n","\n","def create_alpha_cmap(color_input, name=None):\n","    if isinstance(color_input, str):\n","        # If the input is a colormap name\n","        base_cmap = plt.cm.get_cmap(color_input)\n","    elif isinstance(color_input, tuple) and len(color_input) == 3:\n","        if np.max(color_input) > 1:\n","            color_input = (\n","                color_input[0] / 255,\n","                color_input[1] / 255,\n","                color_input[2] / 255,\n","            )\n","        if name is None:\n","            name = f'RGB{color_input}'\n","        base_cmap = mcolors.LinearSegmentedColormap.from_list(name, [color_input, color_input])\n","    else:\n","        raise ValueError(\"Invalid color_input. Must be a colormap name or an RGB tuple.\")\n","\n","    # alpha values ranging from 0 to 1\n","    # tfel: useful to start above zero?\n","    alpha = np.linspace(0.0, 1, base_cmap.N)\n","\n","    colors = base_cmap(np.arange(base_cmap.N))\n","    colors[:, -1] = alpha\n","\n","    alpha_cmap = mcolors.LinearSegmentedColormap.from_list('alpha_cmap', colors)\n","\n","    return alpha_cmap\n","\n","\n","# predefine some alpha cmaps\n","JET_ALPHA = create_alpha_cmap('jet')\n","VIRIDIS_ALPHA = create_alpha_cmap('viridis')\n","\n","_tab_10_colors = [\n","    (31, 119, 180),   # tab:blue\n","    (255, 127, 14),   # tab:orange\n","    (44, 160, 44),    # tab:green\n","    (214, 39, 40),    # tab:red\n","    (148, 103, 189),  # tab:purple\n","    (140, 86, 75),    # tab:brown\n","    (227, 119, 194),  # tab:pink\n","    (127, 127, 127),  # tab:gray\n","    (188, 189, 34),   # tab:olive\n","    (23, 190, 207)    # tab:cyan\n","]\n","\n","TAB10_ALPHA = [create_alpha_cmap(color) for color in _tab_10_colors]\n","\n","cmaps = [google_blue, slack_green, slack_red, slack_yellow, slack_violet, anthracite, google_blue, pink, deep_purple]\n","cmaps = cmaps + TAB10_ALPHA\n","\n","\n","def _get_representative_ids(heatmaps, concept_id, num_images=8):\n","    return np.mean(heatmaps[:, :, :, concept_id], axis=(1, 2)).argsort()[-num_images:]\n","\n","\n","# Overlays heatmaps of the concept in the image\n","def overlay_top_heatmaps(images, heatmaps, concept_id, cmap=None, alpha=0.35, num_images=8):\n","    assert len(images) == len(heatmaps)\n","    assert heatmaps.shape[-1] > concept_id\n","    assert heatmaps.ndim == 4\n","\n","    if cmap is None:\n","        cmap = TAB10_ALPHA[concept_id] if heatmaps.shape[-1] < 10 else VIRIDIS_ALPHA\n","        alpha = 1.0\n","\n","    best_ids = _get_representative_ids(heatmaps, concept_id, num_images)\n","\n","    for i, idx in enumerate(best_ids):\n","        image = images[idx]\n","        heatmap = cv2.resize(heatmaps[idx, :, :, concept_id], (image.shape[1], image.shape[0]), interpolation=cv2.INTER_CUBIC)\n","        #raise NotImplementedError(\"re implement cv2 resize with bicubic\")\n","\n","        plt.subplot(2, int(num_images/2), i + 1)\n","        plt.tight_layout()\n","        show(image)\n","        show(heatmap, cmap=cmap, alpha=alpha)\n","\n","def evidence_top_images(images, heatmaps, concept_id, num_images=8, percentiles=None):\n","    assert len(images) == len(heatmaps)\n","    assert heatmaps.shape[-1] > concept_id\n","    assert heatmaps.ndim == 4\n","\n","    if percentiles is None:\n","        percentiles = np.linspace(50, 95, 10)\n","\n","    best_ids = _get_representative_ids(heatmaps, concept_id, num_images)\n","\n","    for i, idx in enumerate(best_ids):\n","        image = images[idx]\n","        heatmap = cv2.resize(heatmaps[idx, :, :, concept_id], (image.shape[1], image.shape[0]), interpolation=cv2.INTER_CUBIC)\n","\n","        mask = np.zeros_like(heatmap)\n","        for percentile in percentiles:\n","            mask[heatmap > np.percentile(heatmap, percentile)] += 1.0\n","        mask = mask / len(percentiles)\n","\n","        masked_image = image * mask[:, :, None]\n","\n","        plt.subplot(2, int(num_images/2), i + 1)\n","        plt.tight_layout()\n","        show(masked_image.astype(np.uint8))\n","\n","def zoom_top_images(images, heatmaps, concept_id, zoom_size=100, num_images=8):\n","    assert len(images) == len(heatmaps)\n","    assert heatmaps.shape[-1] > concept_id\n","    assert heatmaps.ndim == 4\n","\n","    best_ids = _get_representative_ids(heatmaps, concept_id, num_images)\n","\n","    for i, idx in enumerate(best_ids):\n","        image = images[idx]\n","        heatmap = cv2.resize(heatmaps[idx, :, :, concept_id], (image.shape[1], image.shape[0]), interpolation=cv2.INTER_CUBIC)\n","\n","        hottest_point = np.unravel_index(np.argmax(heatmap, axis=None), heatmap.shape)\n","\n","        x_min = max(hottest_point[0] - zoom_size // 2, 0)\n","        x_max = min(hottest_point[0] + zoom_size // 2, image.shape[0])\n","        y_min = max(hottest_point[1] - zoom_size // 2, 0)\n","        y_max = min(hottest_point[1] + zoom_size // 2, image.shape[1])\n","\n","        zoomed_image = image[x_min:x_max, y_min:y_max]\n","\n","        plt.subplot(2, int(num_images/2), i + 1)\n","        show(zoomed_image)\n","\n","# Produces outlines of each concept\n","def contour_top_image(images, heatmaps, concept_id, percentiles=None, cmap=\"viridis\", linewidth=1.0, num_images=8):\n","    assert len(images) == len(heatmaps)\n","    assert heatmaps.shape[-1] > concept_id\n","    assert heatmaps.ndim == 4\n","\n","    if percentiles is None:\n","        percentiles = [70]\n","\n","    best_ids = _get_representative_ids(heatmaps, concept_id, num_images)\n","\n","    for i, idx in enumerate(best_ids):\n","        image = images[idx]\n","        plt.subplot(2, int(num_images/2), i + 1)\n","        show(image)\n","\n","        heatmap = cv2.resize(heatmaps[idx, :, :, concept_id], (image.shape[1], image.shape[0]), interpolation=cv2.INTER_CUBIC)\n","\n","        for percentile in percentiles:\n","            cut_value = np.percentile(heatmap, percentile)\n","            contours = measure.find_contours(heatmap, cut_value)\n","            for contour in contours:\n","                plt.plot(contour[:, 1], contour[:, 0], linewidth=linewidth, color=plt.get_cmap(cmap)(percentile / 100))\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"32m0Ow-2IqME"},"outputs":[],"source":["def save_craft_results(model_checkpoint, class_idx, concepts, importance, heatmaps):\n","  # Create Directory, change name as needed\n","  dir_name = f\"CRAFT_{class_idx}_class_{heatmaps.shape[0]}_im_{concepts.shape[0]}_concepts_{model_checkpoint}\"\n","  results_dir = os.path.join(project_dir, \"Results\", dir_name)\n","\n","  os.makedirs(results_dir, exist_ok=True)\n","\n","  # Save concepts\n","  np.save(os.path.join(results_dir, \"Concepts.npy\"), concepts)\n","  # Save importances\n","  np.save(os.path.join(results_dir, \"Importances.npy\"), importance)\n","  # Heatmaps (activations)\n","  np.save(os.path.join(results_dir, \"Activations.npy\"), heatmaps)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ENq_rPZgRbk7"},"outputs":[],"source":["import torch\n","from sklearn.decomposition import NMF\n","import torchvision\n","import timm\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","from math import ceil\n","import numpy as np\n","\n","def get_images(data_loader, num_images):\n","    X = []\n","    n = 0\n","    for images, classes, locations in data_loader:\n","        X.append(images)\n","        n += data_loader.batch_size\n","        if n >= num_images:\n","            return X\n","\n","def get_stacked_activations(model, X, num_images):\n","    # Run forward pass\n","    A = []\n","    for batch in X:\n","        # batch = transform(batch)\n","        batch = batch.to('cuda')\n","        batch_A = model.forward_features(batch)\n","        A.append(batch_A.detach().cpu().numpy())\n","\n","    A = np.concatenate(A, axis=0)\n","    return A\n","\n","def load_images_as_np(data_list, num_images=300):\n","  X = []\n","  n = 0\n","  for entry in data_list:\n","    X.append(entry[0])\n","    n += 1\n","    if n == num_images:\n","      return np.array(X)\n","\n","\n","def get_transforms(model):\n","  # Gather transforms\n","  config = timm.data.resolve_model_data_config(model)\n","  transform = timm.data.create_transform(**config, is_training=False,)\n","  return transform"]},{"cell_type":"markdown","metadata":{"id":"Dwq_7gU98oqn"},"source":["##Implementation"]},{"cell_type":"code","execution_count":67,"metadata":{"executionInfo":{"elapsed":519,"status":"ok","timestamp":1733418957756,"user":{"displayName":"Thomas Garity","userId":"11772081689971931307"},"user_tz":300},"id":"K1eGsC7wFZVP"},"outputs":[],"source":["def get_craft_concepts(model, model_checkpoint, class_dir, class_idx, num_concepts=8, num_images=300, results_dir=RESULTS_DIR, visualize=False):\n","  transform = get_transforms(model)\n","\n","\n","  # Create a custom image folder (the Spawrious custom class for datasets)\n","  class_data_list = CustomImageFolder(class_dir,\n","                                    class_index=1, location_index=4,\n","                                    transform=transform, limit =num_images)\n","  no_transforms_list = CustomImageFolder(class_dir,\n","                                    class_index=1, location_index=4,\n","                                    transform=None, limit=num_images)\n","\n","  # Create a dataloader\n","  class_loader = torch.utils.data.DataLoader(\n","      class_data_list,\n","      batch_size=32,\n","      shuffle=False,\n","      num_workers=2,\n","  )\n","\n","  # get images\n","  X = get_images(class_loader, num_images)\n","  A = get_stacked_activations(model, X, num_images)\n","\n","  # Convert X to tensor\n","  X_cat = torch.cat(X, dim=0)\n","  nmf = NMF(num_concepts)\n","\n","  A_flatten = rearrange(A, 'n c h w -> (n h w) c')\n","\n","  Z = nmf.fit_transform(A_flatten)\n","  D = nmf.components_\n","\n","  Z_heatmaps = rearrange(Z, '(n h w) r -> n h w r', n=A.shape[0], h=7, w=7)\n","  # most important are E(Z) * W'\n","  # W' is D @ W.T\n","  W = model.fc.weight.detach().cpu().numpy()\n","  W_p = D @ W.T\n","  Importance = np.mean(Z, 0) * W_p[:, class_idx]\n","\n","  # Plot concepts\n","  Imgs = load_images_as_np(no_transforms_list, num_images)\n","\n","  for concept_id in np.argsort(Importance)[::-1]:\n","\n","    set_size(8, 4)\n","\n","    overlay_top_heatmaps(Imgs[:A.shape[0]], Z_heatmaps, concept_id, cmap=cmaps[concept_id], alpha=1.0)\n","    plt.suptitle(f\"Concept {concept_id}; Importance:{Importance[concept_id]:.4f}\")\n","    plt.tight_layout()\n","    plt.axis('off')\n","\n","    # always save the concepts as pngs: save to a specifically-name dir\n","    dir_name = f\"CRAFT_{class_idx}_class_{num_images}_im_{D.shape[0]}_concepts_{model_checkpoint}\"\n","    plot_name = f\"concept_{concept_id}_importance_{Importance[concept_id]:.4f}.png\"\n","    plot_path = os.path.join(results_dir, dir_name, \"concept_images\", plot_name)\n","    os.makedirs(os.path.join(results_dir, dir_name), exist_ok=True)\n","    os.makedirs(os.path.join(results_dir, dir_name, \"concept_images\"), exist_ok=True)\n","\n","\n","    plt.savefig(plot_path, dpi=300)\n","\n","    if visualize:\n","      plt.show()\n","      print(\"\\n\")\n","\n","\n","  # Save results\n","  save_craft_results(model_checkpoint, class_idx, D, Importance, Z_heatmaps)\n","  return dir_name\n"]},{"cell_type":"code","execution_count":68,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1RAqHUgg0N8ZANzWIyqQl7ywcBGbAjQ3u"},"executionInfo":{"elapsed":27803,"status":"ok","timestamp":1733418988179,"user":{"displayName":"Thomas Garity","userId":"11772081689971931307"},"user_tz":300},"id":"woPp0UwCJzjc","outputId":"448f891f-9076-4a9a-9c42-b9163740a36c"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["get_craft_concepts(normal_model, normal_model_name, \"/content/data/spawrious224/0/desert/bulldog\", 0, num_concepts=8, num_images=10, results_dir=RESULTS_DIR, visualize=True)"]},{"cell_type":"markdown","metadata":{"id":"vVgaIbiNu_ix"},"source":["# 4. Load Concepts"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GgQGqAeIvB_u"},"outputs":[],"source":["# Custom class for storing concept results\n","class ConceptResults():\n","\n","    def __init__(self, concept_instance_name):\n","        concept_results_dir = os.path.join(RESULTS_DIR, concept_instance_name)\n","        self.concepts = np.load(os.path.join(concept_results_dir, \"Concepts.npy\"))\n","        self.importances = np.load(os.path.join(concept_results_dir, \"Importances.npy\")),\n","        self.activations = np.load(os.path.join(concept_results_dir, \"Activations.npy\")),\n","\n","\n","\n","def load_concept_results(concept_instance_name):\n","    \"\"\"Load pre-computed concept results from the results directory\"\"\"\n","    concept_results_dir = os.path.join(RESULTS_DIR, concept_instance_name)\n","\n","    results = {\n","        'concepts': np.load(os.path.join(concept_results_dir, \"Concepts.npy\")),\n","        'importances': np.load(os.path.join(concept_results_dir, \"Importances.npy\")),\n","        'activations': np.load(os.path.join(concept_results_dir, \"Activations.npy\")),\n","    }\n","\n","    return results\n"]},{"cell_type":"markdown","metadata":{"id":"ypC3MAdQFBdZ"},"source":["# 5. Run Experiments"]},{"cell_type":"markdown","metadata":{"id":"bHD4DGwHGbpT"},"source":["## Helper Functions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5MKJaWK4FN_y"},"outputs":[],"source":["# Convert images from a dataloader into a list of tensors\n","def get_images(data_loader, num_images):\n","    X = []\n","    n = 0\n","    for images, classes, locations in data_loader:\n","        X.append(images)\n","        n += data_loader.batch_size\n","        if n >= num_images:\n","            return X\n","\n","# Store the stacked activations of\n","def get_stacked_activations(model, X, num_images):\n","    # Run forward pass\n","    A = []\n","    for batch in X:\n","        batch = get_transforms(model)(batch)\n","        batch = batch.to('cuda')\n","        batch_A = model.forward_features(batch)\n","        A.append(batch_A.detach().cpu().numpy())\n","\n","    A = np.concatenate(A, axis=0)\n","    return A\n","\n","# Helper : computes the cost of an intermediary nmf solution\n","def nmf_cost(H_vec, W, A_T):\n","    k, n = A_T.shape[0], W.shape[0]\n","    H = H_vec.reshape(k, n)\n","\n","    reconstruction = H @ W\n","    return np.linalg.norm(A_T - reconstruction, 'fro')**2\n","\n","\n","# Get Activations of a set of concepts when passing through a new set of images\n","def get_concept_activations(stacked_activations, concepts):\n","\n","    # Flatten the activations\n","    A_flatten = rearrange(stacked_activations, 'n c h w -> (n h w) c')\n","\n","    # Initialize random guess for concept activations\n","    concept_activation_init = np.random.rand(A_flatten.shape[0], concepts.shape[0])\n","    print(\"H init\", concept_activation_init.shape)\n","    concept_activation_init = concept_activation_init.flatten()\n","\n","    # Solve with constraint H' >= 0\n","    result = minimize(nmf_cost, concept_activation_init,\n","                      args=(concepts, A_flatten),\n","                      method='L-BFGS-B',\n","                      bounds=[(0, None)] * concept_activation_init.size,\n","                      options={'maxiter':200,'disp': True,})\n","    concept_activations = result.x.reshape(concepts.shape[1], -1)\n","\n","    return concept_activations\n","\n","def get_concept_activations_gpu(stacked_activations, concepts):\n","\n","    # Flatten the activations\n","    A_flatten = rearrange(stacked_activations, 'n c h w -> (n h w) c')\n","\n","    # Move matrices to GPU\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    A_torch = torch.tensor(A_flatten, dtype=torch.float32).to(device)  # Transposed A\n","    W_torch = torch.tensor(concepts, dtype=torch.float32).to(device)\n","    # H_prime: randomly-initialized matrix for the concept activations\n","    H_prime = torch.nn.Parameter(torch.rand(A_torch.shape[0], W_torch.shape[0], device=device, dtype=torch.float32))\n","\n","    # Checkout shapes of matrices\n","    # print(f\"Activations: {A_torch.shape}, concepts:  {W_torch.shape}, concept activations: {H_prime.shape}\")\n","\n","    # Define loss function (Frobenius norm)\n","    loss_fn = nn.MSELoss()\n","\n","    optimizer = torch.optim.LBFGS([H_prime], max_iter=300)\n","\n","    def closure():\n","        optimizer.zero_grad()\n","        reconstruction =  H_prime @ W_torch\n","        loss = loss_fn(reconstruction, A_torch)\n","        loss.backward()\n","        return loss\n","\n","    # Run optimization\n","    optimizer.step(closure)\n","\n","    # Convert back to NumPy if needed\n","    H_prime_result = H_prime.detach().cpu().numpy()\n","    return H_prime_result\n","\n","\n","# Return concept importances\n","def get_concept_importances(model, concepts, concept_activations, class_idx):\n","    # Pull final layer weights from model\n","    W = model.fc.weight.detach().cpu().numpy()\n","\n","    # Multiply those by the concepts -- this provides the importance of each concept\n","    W_p = concepts @ W.T\n","\n","    # Average the weights * activations specifically for the target class\n","    importances = np.mean(concept_activations, 0) * W_p[:, class_idx] # corgi class\n","\n","    return importances"]},{"cell_type":"markdown","metadata":{"id":"F-RHZvwamLd6"},"source":["## First, check that models are spurious"]},{"cell_type":"code","execution_count":183,"metadata":{"executionInfo":{"elapsed":522,"status":"ok","timestamp":1733424745612,"user":{"displayName":"Thomas Garity","userId":"11772081689971931307"},"user_tz":300},"id":"b4GJstJVxU1h"},"outputs":[],"source":["# Eval loop\n","def evaluate(model: Module, loader: DataLoader, device: torch.device) -> float:\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for inputs, labels, _ in tqdm(\n","            loader, desc=\"Evaluating\", leave=False\n","        ):  # third item is the location label\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            outputs = model(inputs)\n","            _, predicted = torch.max(outputs.data, 1)\n","\n","            #print(\"\\nPredictions: \",  predicted, \"\\nTruth: \", labels)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","    acc = 100 * correct / total\n","    return acc\n","\n","# Return the class index associated with a specific location\n","def get_class_index(model_name, location):\n","  # Extract the trainig dataset type from the model_name\n","  if \"o2o\" in model_name:\n","    train_data_type = \"o2o_\"+model_name.split(\"o2o_\")[1].split(\"_\")[0]\n","  else:\n","    train_data_type = \"entire_dataset\"\n","\n","  # Mapping of (idx in the combinations below) : (idx of class)\n","  # example: corgi uses the \"snow\" class in o2o_easy (3rd index), but is has class_idx 1\n","  class_index_mappings = {0: 0, 1: 2, 2: 3, 3: 1}\n","\n","  combinations = {\n","        \"o2o_easy\": (\n","            [\"desert\", \"jungle\", \"dirt\", \"snow\"],\n","        ),\n","\n","        \"entire_dataset\": (\n","            [\"desert\", \"jungle\", \"dirt\", \"snow\"],\n","        ),\n","\n","        \"o2o_medium\": (\n","            [\"mountain\", \"beach\", \"dirt\", \"jungle\"],\n","        ),\n","        \"o2o_hard\": (\n","            [\"jungle\", \"mountain\", \"snow\", \"desert\"],\n","        ),\n","    }\n","\n","  try:\n","    return class_index_mappings[combinations[train_data_type][0].index(location)]\n","  except:\n","    return -1\n","\n","# Test the model on a background-only dataset\n","def bg_only_test(model, model_name, bg_dir, num_images=100):\n","\n","  data_lists = []\n","  # Create a custom image folder (the Spawrious custom class for datasets)\n","  for location_dir in os.listdir(bg_dir):\n","      location = location_dir.split('_')[0]\n","      location_dir = os.path.join(bg_dir, location_dir)\n","\n","      # Index into combinations to get the class index associated with the background\n","      class_index = get_class_index(model_name, location)\n","      if class_index == -1:\n","        continue\n","\n","      background_data_list = CustomImageFolder(location_dir,\n","                                        class_index=class_index, location_index=0,\n","                                        transform = get_transforms(model), limit = num_images)\n","      # Add it to the list of datasets\n","      data_lists.append(background_data_list)\n","\n","\n","  # Concatenate all of the datasets\n","  combined_dataset = torch.utils.data.ConcatDataset(data_lists)\n","\n","  # Create a dataloader\n","  bg_loader = torch.utils.data.DataLoader(\n","      combined_dataset,\n","      batch_size=32,\n","      shuffle=False,\n","      num_workers=2,\n","  )\n","\n","  return evaluate(model=model, loader=bg_loader, device=torch.device(\"cuda\"))"]},{"cell_type":"markdown","metadata":{"id":"iSqG0_2ZFDMt"},"source":["## Method 1: Testing Background Concept Importance"]},{"cell_type":"code","execution_count":102,"metadata":{"executionInfo":{"elapsed":512,"status":"ok","timestamp":1733420856177,"user":{"displayName":"Thomas Garity","userId":"11772081689971931307"},"user_tz":300},"id":"L80quGjmM-m5"},"outputs":[],"source":["# Filter out background concepts, using activations on the class dataset and background-only dataset\n","def filter_background_concepts(activations, background_activations, concepts, num_images, cutoff = 0.5, method='mean', verbose=False):\n","\n","    # Iterate through concepts\n","    background_concepts = []\n","\n","    # Dict for saving results\n","    results = {\"concept\" : [],\n","              \"class_activation\" : [],\n","              \"background_activation\" : [],\n","              \"ratio\" : [],\n","               \"background\" : []\n","             }\n","\n","    for concept_id in range(len(concepts)):\n","        if method == 'mean':\n","\n","            # Compute mean activation of that concept\n","            activation_aggregated = activations[..., concept_id].mean()\n","            bg_activation_aggregated = background_activations[..., concept_id].mean()\n","\n","        if method == 'median':\n","\n","            # Compute median activation of that concept\n","            activation_aggregated = np.median(activations[..., concept_id])\n","            bg_activation_aggregated = np.median(background_activations[..., concept_id])\n","\n","        if method == 'med-mean':\n","\n","            # Take mean of each 7*7 activation map, then median of those means\n","            activation_aggregated = np.median(np.mean(activations[..., concept_id], axis=(1,2)))\n","            bg_activation_aggregated = np.median(np.mean(background_activations[..., concept_id], axis=(1,2)))\n","\n","\n","        # If the concept is more activated by the background, then it\n","        # likely does not have anything to do with the class\n","        ratio = activation_aggregated / bg_activation_aggregated\n","        if ratio < cutoff:\n","            background_concepts.append(concept_id)\n","\n","        # Update the results dict\n","        results[\"concept\"].append(concept_id)\n","        results[\"class_activation\"].append(activation_aggregated)\n","        results[\"background_activation\"].append(bg_activation_aggregated)\n","        results[\"ratio\"].append(ratio)\n","        results[\"background\"].append(ratio < cutoff)\n","\n","\n","\n","\n","    # Return concepts\n","    return background_concepts, pd.DataFrame(results)\n","\n","def score(importances, bg_concepts):\n","  background_concept_importances = np.linalg.norm(importances[bg_concepts])\n","  return background_concept_importances / np.linalg.norm(importances)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vNpBcAwOXh0D"},"outputs":[],"source":["def show_concepts(heatmaps, importances, imgs):\n","  for concept_id in np.argsort(importances)[::-1]:\n","    set_size(10, 3)\n","    overlay_top_heatmaps(imgs[:heatmaps.shape[0]], heatmaps, concept_id, cmap=cmaps[concept_id], alpha=1.0)\n","    print(f'Concept {concept_id} Importance score', importances[concept_id])\n","    plt.tight_layout()\n","    plt.show()\n","\n","\n","def compare_concept(concept_id, heatmaps, bg_heatmaps, imgs, bg_imgs, importances):\n","    set_size(10, 3)\n","    overlay_top_heatmaps(imgs[:heatmaps.shape[0]], heatmaps, concept_id, alpha=1.0, num_images=6)\n","    #zoom_top_images(imgs[:heatmaps.shape[0]], heatmaps, concept_id)\n","\n","    print(f'Class Data: Concept {concept_id} Importance score', importances[concept_id])\n","    plt.tight_layout()\n","    plt.show()\n","    overlay_top_heatmaps(bg_imgs[:heatmaps.shape[0]], bg_heatmaps, concept_id, alpha=1.0, num_images=6)\n","    #zoom_top_images(bg_imgs[:heatmaps.shape[0]], bg_heatmaps, concept_id)\n","\n","    print(f'Background Data: Concept {concept_id} Importance score', importances[concept_id])\n","    plt.tight_layout()\n","    plt.show()"]},{"cell_type":"code","execution_count":111,"metadata":{"executionInfo":{"elapsed":547,"status":"ok","timestamp":1733421218080,"user":{"displayName":"Thomas Garity","userId":"11772081689971931307"},"user_tz":300},"id":"NsGgStt4XS6t"},"outputs":[],"source":["def experiment(bugged_model, normal_model,\n","               class_data_dir, bg_data_dir,\n","               bugged_concept_results, normal_concept_results, class_id,\n","               num_images = 100, show_images = True, method='med-mean', cutoff = 1.0):\n","\n","  # Create a custom image folder (the Spawrious custom class for datasets)\n","  class_data_list = CustomImageFolder(class_data_dir,\n","                                    class_index=1, location_index=4,\n","                                    transform = get_transforms(bugged_model), limit = num_images)\n","  # Create custom image folder for snow-only data ; these are also labeled as corgi\n","  background_data_list = CustomImageFolder(bg_data_dir,\n","                                    class_index=1, location_index=4,\n","                                    transform = get_transforms(bugged_model), limit = num_images)\n","\n","\n","  # Create a dataloader\n","  corgi_loader = torch.utils.data.DataLoader(\n","      class_data_list,\n","      batch_size=32,\n","      shuffle=False,\n","      num_workers=2,\n","  )\n","\n","  # Create background dataloader\n","  bg_loader = torch.utils.data.DataLoader(\n","      background_data_list,\n","      batch_size=32,\n","      shuffle=False,\n","      num_workers=2,\n","  )\n","\n","  # Store image files without transforms if we will be plotting them\n","  if show_images:\n","      class_no_transforms_list = CustomImageFolder(class_data_dir,\n","                                    class_index=class_id, location_index=4,\n","                                    transform = None, limit = num_images)\n","      bg_no_transforms_list = CustomImageFolder(bg_data_dir,\n","                                    class_index=class_id, location_index=4,\n","                                    transform = None, limit = num_images)\n","\n","      imgs = load_images_as_np(class_no_transforms_list, num_images)\n","      imgs_bg = load_images_as_np(bg_no_transforms_list, num_images)\n","\n","\n","  spurious_scores = []\n","\n","  concepts_dfs = []\n","\n","  for model, concept_results in zip([bugged_model, normal_model], [bugged_concept_results, normal_concept_results]):\n","\n","    if show_images:\n","      if(model == bugged_model):\n","        print(\"Bugged model\", \"-\"*60)\n","      else:\n","        print(\"Normal model\", \"-\"*60)\n","\n","\n","    # Get the model's FEATURE actviations on corgi images\n","    X = get_images(corgi_loader, num_images)\n","    A = get_stacked_activations(model, X, len(X))\n","\n","    # Get the models' FEATURE activations on background images\n","    X_bg = get_images(bg_loader, num_images)\n","    A_bg = get_stacked_activations(model, X_bg, len(X))\n","\n","\n","    # get CONCEPT activations using GPU\n","    bg_activations = get_concept_activations_gpu(A_bg, concept_results.concepts)\n","    activations = get_concept_activations_gpu(A, concept_results.concepts)\n","\n","    # Calculate differences\n","    bg_importances = get_concept_importances(model, concept_results.concepts, bg_activations, 1)\n","    importances = get_concept_importances(model, concept_results.concepts, activations, 1)\n","\n","    # print(f\"\\nImportances in background dataset: \\n{bg_importances},\\nimportances in original dataset: \\n{importances}\")\n","\n","    bg_heatmaps = rearrange(bg_activations, '(n h w) r -> n h w r', n=A.shape[0], h=7, w=7)\n","    heatmaps = rearrange(activations, '(n h w) r -> n h w r', n=A.shape[0], h=7, w=7)\n","\n","    bg_concepts, concepts_df = filter_background_concepts(heatmaps, bg_heatmaps, concept_results.concepts, num_images, cutoff, method=method, verbose=show_images)\n","    concepts_dfs.append(concepts_df)\n","    spurious_scores.append(score(importances, bg_concepts))\n","\n","    if show_images:\n","      print(concepts_df)\n","\n","      print(\"\\n\\nComparing Background Concepts\")\n","      for concept_id in bg_concepts:\n","        compare_concept(concept_id, heatmaps, bg_heatmaps, imgs, imgs_bg, importances)\n","\n","      print(\"\\n\\nComparing Class Concepts\")\n","      for concept_id in range(len(concept_results.concepts)):\n","        if concept_id not in bg_concepts:\n","          compare_concept(concept_id, heatmaps, bg_heatmaps, imgs, imgs_bg, importances)\n","  print(spurious_scores)\n","  return spurious_scores, concepts_dfs"]},{"cell_type":"code","execution_count":84,"metadata":{"executionInfo":{"elapsed":527,"status":"ok","timestamp":1733420382212,"user":{"displayName":"Thomas Garity","userId":"11772081689971931307"},"user_tz":300},"id":"W-uosWnwT_-0"},"outputs":[],"source":["def single_model_experiment(model,\n","               class_data_dir, bg_data_dir,\n","               concept_results, class_idx,\n","               num_images = 100, show_images = True, method='med-mean', cutoff = 1.0):\n","\n","  # Create a custom image folder (the Spawrious custom class for datasets)\n","  class_data_list = CustomImageFolder(class_data_dir,\n","                                    class_index=1, location_index=4,\n","                                    transform = get_transforms(model), limit = num_images)\n","  # Create custom image folder for snow-only data ; these are also labeled as corgi\n","  background_data_list = CustomImageFolder(bg_data_dir,\n","                                    class_index=1, location_index=4,\n","                                    transform = get_transforms(model), limit = num_images)\n","\n","\n","  # Create a dataloader\n","  corgi_loader = torch.utils.data.DataLoader(\n","      class_data_list,\n","      batch_size=32,\n","      shuffle=False,\n","      num_workers=2,\n","  )\n","\n","  # Create background dataloader\n","  bg_loader = torch.utils.data.DataLoader(\n","      background_data_list,\n","      batch_size=32,\n","      shuffle=False,\n","      num_workers=2,\n","  )\n","\n","  # Store image files without transforms if we will be plotting them\n","  if show_images:\n","      class_no_transforms_list = CustomImageFolder(class_data_dir,\n","                                    class_index=class_idx, location_index=4,\n","                                    transform = None, limit = num_images)\n","      bg_no_transforms_list = CustomImageFolder(bg_data_dir,\n","                                    class_index=class_idx, location_index=4,\n","                                    transform = None, limit = num_images)\n","\n","      imgs = load_images_as_np(class_no_transforms_list, num_images)\n","      imgs_bg = load_images_as_np(bg_no_transforms_list, num_images)\n","\n","\n","  spurious_scores = []\n","\n","  # Get the model's FEATURE actviations on class images\n","  X = get_images(corgi_loader, num_images)\n","  A = get_stacked_activations(model, X, num_images)\n","\n","  # Get the models' FEATURE activations on background images\n","  X_bg = get_images(bg_loader, num_images)\n","  A_bg = get_stacked_activations(model, X_bg, num_images)\n","\n","\n","  # get CONCEPT activations using GPU\n","  bg_activations = get_concept_activations_gpu(A_bg, concept_results.concepts)\n","  activations = get_concept_activations_gpu(A, concept_results.concepts)\n","\n","  # Calculate differences\n","  bg_importances = get_concept_importances(model, concept_results.concepts, bg_activations, class_idx)\n","  importances = get_concept_importances(model, concept_results.concepts, activations, class_idx)\n","\n","  # print(f\"\\nImportances in background dataset: \\n{bg_importances},\\nimportances in original dataset: \\n{importances}\")\n","\n","  bg_heatmaps = rearrange(bg_activations, '(n h w) r -> n h w r', n=num_images, h=7, w=7)\n","  heatmaps = rearrange(activations, '(n h w) r -> n h w r', n=num_images, h=7, w=7)\n","\n","  bg_concepts, concepts_df = filter_background_concepts(heatmaps, bg_heatmaps, concept_results.concepts, num_images, cutoff, method=method, verbose=show_images)\n","  spurious_scores.append(score(importances, bg_concepts))\n","\n","  if show_images:\n","    print(concepts_df)\n","\n","    print(\"\\n\\nComparing Background Concepts\")\n","    for concept_id in bg_concepts:\n","      compare_concept(concept_id, heatmaps, bg_heatmaps, imgs, imgs_bg, importances)\n","\n","    print(\"\\n\\nComparing Class Concepts\")\n","    for concept_id in range(len(concept_results.concepts)):\n","      if concept_id not in bg_concepts:\n","        compare_concept(concept_id, heatmaps, bg_heatmaps, imgs, imgs_bg, importances)\n","  print(spurious_scores)\n","  return spurious_scores, concepts_df"]},{"cell_type":"code","source":["# Helper returns the name of the concept results instance w given inputs\n","def get_concept_results_name(model_checkpoint, class_idx,  num_images, num_concepts):\n","  return f\"CRAFT_{class_idx}_class_{num_images}_im_{num_concepts}_concepts_{model_checkpoint}\"\n","\n","def create_or_load_concepts(model, model_path_name, class_dir, class_id, num_concepts):\n","  # Check for existing concepts, generate them if not\n","  try:\n","    craft_results_dir = get_concept_results_name(model_path_name, class_id, num_images=300, num_concepts=num_concepts)\n","    return ConceptResults(craft_results_dir)\n","  except(FileNotFoundError):\n","    print(\"No existing concepts found...\")\n","    print(\"Generating concepts...\")\n","    craft_results_dir = get_craft_concepts(model, model_path_name, class_dir,\n","                                          class_id, num_concepts=num_concepts, num_images=300,\n","                                          results_dir=RESULTS_DIR, visualize=False)\n","    print(\"Concept generation complete: \", craft_results_dir)\n","  return ConceptResults(craft_results_dir)\n"],"metadata":{"id":"qVbMd806912T","executionInfo":{"status":"ok","timestamp":1733422946576,"user_tz":300,"elapsed":520,"user":{"displayName":"Thomas Garity","userId":"11772081689971931307"}}},"execution_count":154,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CMveYnpeQJKh"},"source":["#6. Run ENTIRE Pipeline (concept generation, spurious test, spurious scores)"]},{"cell_type":"code","execution_count":181,"metadata":{"executionInfo":{"elapsed":517,"status":"ok","timestamp":1733424643378,"user":{"displayName":"Thomas Garity","userId":"11772081689971931307"},"user_tz":300},"id":"cjjDkq89QIp-"},"outputs":[],"source":["def run_pipeline(model_path_name, bg_dataset_dir, normal_dataset_dir, single_model=True):\n","\n","  all_results = {}\n","\n","  model = load_model(model_path_name)\n","  normal_model = load_model(normal_model_name)\n","\n","  locations = [\"desert\", \"dirt\", \"jungle\", \"snow\", \"mountain\"]\n","\n","  class_list = [\"bulldog\", \"corgi\", \"dachshund\", \"labrador\"]\n","\n","\n","  scores = {}\n","  # Test the spurious-ness of the model; save the results\n","  scores['bugged'] = [bg_only_test(model, model_path_name, bg_dataset_dir)]\n","  print(\"Bugged Model Accuracy on background-only dataset: \", scores['bugged'])\n","  scores['normal'] = [bg_only_test(normal_model, model_path_name, bg_dataset_dir)]\n","  print(\"Normal Model Accuracy on background-only dataset: \", scores['normal'])\n","\n","  output_dir = os.path.join(EXPERIMENT_OUTPUT_DIR, model_path_name)\n","  print(\"Saving Results to: \", output_dir)\n","  os.makedirs(output_dir, exist_ok=True)\n","\n","  df = pd.DataFrame(scores)\n","  df.to_csv(os.path.join(output_dir, \"background_only_accuracies.csv\"))\n","\n","  for location in locations:\n","    print(location)\n","\n","    # Get class id and name\n","    class_id = get_class_index(model_path_name, location)\n","    print(class_id)\n","    class_name = class_list[class_id]\n","\n","    print(f\"Testing on {class_name} / {location}\", \"-\"*90)\n","    continue\n","    # Set dirs\n","    class_dir = os.path.join(normal_dataset_dir,\"0\", location, class_name)\n","    bg_dir = os.path.join(bg_dataset_dir, location+\"_only\")\n","\n","\n","    # Dicts for storing results\n","    results = pd.DataFrame(columns=['method_model', 'spurious_score', 'num_concepts'])\n","    accuracies = pd.DataFrame(columns=['num_concepts', 'accuracy', 'method'])\n","\n","    for num_concepts in range(4, 12, 3):\n","      print(f\"  Number of Concepts: {num_concepts}\", \"-\"*50)\n","      concept_results = create_or_load_concepts(model, model_path_name, class_dir, class_id, num_concepts)\n","      normal_concept_results = create_or_load_concepts(normal_model, normal_model_name, class_dir, class_id, num_concepts)\n","\n","      # Gather spurious score\n","      num_trials = 1\n","\n","      for method in ['med-mean', 'mean', 'median']:\n","        print(f\"Method: {method}\", \"-\"*20)\n","\n","        correct = 0\n","\n","        for i in range (num_trials):\n","\n","          if single_model:\n","            scores, concepts_df = single_model_experiment(model, class_dir,\n","                                bg_dir, concept_results, class_id, num_images=10,\n","                                show_images = False, method = method, cutoff = 1.0)\n","            concepts_df.to_csv(os.path.join(output_dir, f\"{class_name}_{location}_{num_concepts}_concepts.csv\"))\n","\n","          else:\n","            scores, concepts_dfs = experiment(model, normal_model, class_dir,\n","                                bg_dir, concept_results, normal_concept_results,\n","                                class_id, num_images=10,\n","                                show_images = False, method = method, cutoff = 1.0)\n","\n","            concepts_dfs[0].to_csv(os.path.join(output_dir, f\"{class_name}_{location}_bugged_{num_concepts}_concepts.csv\"))\n","            concepts_dfs[1].to_csv(os.path.join(output_dir, f\"{class_name}_{location}_normal_{num_concepts}_concepts.csv\"))\n","\n","          # Update results df\n","          new_results = pd.DataFrame({'method_model': [f\"{method}_bugged\", f\"{method}_normal\"],\n","                                      'spurious_score': scores, 'num_concepts': [num_concepts, num_concepts]})\n","          if results.empty:\n","            results = new_results\n","          else:\n","            results = pd.concat([results, new_results], ignore_index=True)\n","\n","          # Update prediction accuracy : this measures how well our metric predicts a buggy model\n","          pred = np.argmax(scores)\n","          correct += (pred == 0)\n","\n","\n","        new_accuracy = pd.DataFrame({'method': method, 'accuracy': [correct / num_trials], 'num_concepts': [num_concepts]})\n","        # Update accuracy df\n","        if accuracies.empty:\n","            accuracies = new_accuracy\n","        else:\n","            accuracies = pd.concat([accuracies, new_accuracy], ignore_index=True)\n","\n","    # Save spurious scores\n","    results.to_csv(os.path.join(output_dir, f\"{class_name}_{location}_Spurious_scores.csv\"))\n","    # Save accuracies\n","    accuracies.to_csv(os.path.join(output_dir, f\"{class_name}_{location}_Spurious_Detection_Accuracies.csv\"))\n","\n","  return output_dir"]},{"cell_type":"code","execution_count":184,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4719,"status":"ok","timestamp":1733424755044,"user":{"displayName":"Thomas Garity","userId":"11772081689971931307"},"user_tz":300},"id":"0GxzNCrnWEqK","outputId":"4c4d87e1-0151-4f65-b2c1-4a19fa6a66ed"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-9-a9afaae834bd>:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model_weights = torch.load(os.path.join(models_dir, checkpoint_path))\n"]},{"output_type":"stream","name":"stdout","text":["Loc:  /content/data/background_only_dataset/jungle_only cls:  1\n","Loc:  /content/data/background_only_dataset/dirt_only cls:  3\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Bugged Model Accuracy on background-only dataset:  [27.5]\n","Loc:  /content/data/background_only_dataset/jungle_only cls:  1\n","Loc:  /content/data/background_only_dataset/dirt_only cls:  3\n"]},{"output_type":"stream","name":"stderr","text":["                                                         "]},{"output_type":"stream","name":"stdout","text":["Normal Model Accuracy on background-only dataset:  [45.0]\n","Saving Results to:  /content/drive/MyDrive/CS2822_Final_Project/Final_Results/o2o_medium_resnet18.a1-e=2-lr=0.01_limit=20.pt\n","desert\n","-1\n","Testing on labrador / desert ------------------------------------------------------------------------------------------\n","dirt\n","3\n","Testing on labrador / dirt ------------------------------------------------------------------------------------------\n","jungle\n","1\n","Testing on corgi / jungle ------------------------------------------------------------------------------------------\n","snow\n","-1\n","Testing on labrador / snow ------------------------------------------------------------------------------------------\n","mountain\n","0\n","Testing on bulldog / mountain ------------------------------------------------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["\r"]}],"source":["output_dir = run_pipeline(bugged_model_name, \"/content/data/background_only_dataset\", \"/content/data/spawrious224\", single_model=False)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[],"collapsed_sections":["3X1_OWD1DM6j","5z0eDI3rDPmm","axvRnmqoDU2B","rK07iVJDsFi1","vVgaIbiNu_ix","bHD4DGwHGbpT","F-RHZvwamLd6"],"mount_file_id":"10tsWUC60ZVN3AZJzfhUUvHwXo-eGPwOa","authorship_tag":"ABX9TyMvFUUTYwnx0Seq3m75JYQk"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"3717b759ff23449dadb62c60d3fe4f8d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4616a8ae8004437596ac51878db06f1c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d6ffc141854a41a79e82d68cadfd0e7e","placeholder":"","style":"IPY_MODEL_5d0ef6b7cb934376a45b9edc9de228a1","value":"model.safetensors:100%"}},"5d0ef6b7cb934376a45b9edc9de228a1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5dd888647003455391466a1f7aaaa37a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4616a8ae8004437596ac51878db06f1c","IPY_MODEL_75e55fecf8aa48e88b603a0c6230ab84","IPY_MODEL_af2eb4c3d9de4160b3992638bad61afc"],"layout":"IPY_MODEL_f9af26d8401041598548e3f064a802dc"}},"60c15e34fa9542f0918df03baa91ee49":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"75e55fecf8aa48e88b603a0c6230ab84":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_60c15e34fa9542f0918df03baa91ee49","max":46807446,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3717b759ff23449dadb62c60d3fe4f8d","value":46807446}},"af2eb4c3d9de4160b3992638bad61afc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f4bfe880b05049fc87131ee6c7b7e55e","placeholder":"","style":"IPY_MODEL_ead64455521c4522852cd087aa7a967f","value":"46.8M/46.8M[00:00&lt;00:00,180MB/s]"}},"d6ffc141854a41a79e82d68cadfd0e7e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ead64455521c4522852cd087aa7a967f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f4bfe880b05049fc87131ee6c7b7e55e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f9af26d8401041598548e3f064a802dc":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat":4,"nbformat_minor":0}