{"cells":[{"cell_type":"markdown","metadata":{"id":"fgBYcvv3eg36"},"source":["Install Dependencies"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23683,"status":"ok","timestamp":1731467176489,"user":{"displayName":"Thomas Garity","userId":"01070805114987455925"},"user_tz":300},"id":"3WIwwUl9fHKp","outputId":"49648d05-c44b-4eb3-94c5-a54d04c66f7a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting git+https://github.com/aengusl/spawrious.git\n","  Cloning https://github.com/aengusl/spawrious.git to /tmp/pip-req-build-dflwd99h\n","  Running command git clone --filter=blob:none --quiet https://github.com/aengusl/spawrious.git /tmp/pip-req-build-dflwd99h\n","  Resolved https://github.com/aengusl/spawrious.git to commit 651e90415bfb414a5b6579cb1f53b83e6bb8c165\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: spawrious\n","  Building wheel for spawrious (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for spawrious: filename=spawrious-0.1.0-py3-none-any.whl size=12565 sha256=0af6caee2b3abf0cfb68181f7634cfdee3da1be2d1d3a9f1533397549eeeff9f\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-ucolfd62/wheels/d3/4b/f5/3f325090cd132ceb57c7ebb25451aa140060b2f50ea24d6c8c\n","Successfully built spawrious\n","Installing collected packages: spawrious\n","Successfully installed spawrious-0.1.0\n"]}],"source":["#!pip install git+https://github.com/aengusl/spawrious.git"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3TRsaZ8NejBy"},"outputs":[],"source":["import argparse\n","\n","import torch\n","import torch.optim as optim\n","from torch import nn\n","from torch.nn import Module\n","from torch.optim import Optimizer\n","from torch.utils.data import DataLoader\n","from torchvision import models\n","from tqdm import tqdm\n","from tqdm.auto import tqdm\n","import timm\n","import wandb\n","import os"]},{"cell_type":"markdown","metadata":{"id":"sTxifmjzeNWh"},"source":["#0. Spawrious Source Code (For Editing)\n","\n","Edited so to use fewer images per folder\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mvZeSRoAeLh-"},"outputs":[],"source":["import os\n","import tarfile\n","import urllib\n","import urllib.request\n","from typing import Any, Tuple\n","\n","import torch\n","from PIL import Image\n","from torch.utils.data import ConcatDataset, Dataset\n","from torchvision import transforms\n","from torchvision.datasets import ImageFolder\n","from tqdm import tqdm\n","import timm\n","from PIL import ImageFile\n","\n","ImageFile.LOAD_TRUNCATED_IMAGES = True\n","\n","# MODEL_NAME = \"vit_so400m_patch14_siglip_384\"\n","# MODEL_NAME = 'swin_base_patch4_window7_224.ms_in22k_ft_in1k'\n","# MODEL_NAME = 'deit3_base_patch16_224.fb_in22k_ft_in1k'\n","# MODEL_NAME = 'beit_base_patch16_224.in22k_ft_in22k_in1k'\n","# MODEL_NAME = 'eva02_base_patch14_448.mim_in22k_ft_in22k_in1k'\n","# MODEL_NAME = 'levit_128s.fb_dist_in1k'\n","\n","MODEL_NAME = None\n","\n","def set_model_name(name):\n","    global MODEL_NAME\n","    MODEL_NAME = name\n","\n","\n","def _extract_dataset_from_tar(\n","    tar_file_name: str, data_dir: str\n",") -> None:\n","    tar_file_dst = os.path.join(data_dir, tar_file_name)\n","    print(\"Extracting dataset...\")\n","    tar = tarfile.open(tar_file_dst, \"r:gz\")\n","    tar.extractall(os.path.dirname(tar_file_dst))\n","    tar.close()\n","\n","\n","def _download_dataset_if_not_available(\n","    dataset_name: str, data_dir: str, remove_tar_after_extracting: bool = True\n",") -> None:\n","    \"\"\"\n","    datasets.txt file, which is present in the data_dir, is used to check if the dataset is already extracted. If the dataset is already extracted, then the tar file is not downloaded again.\n","    \"\"\"\n","    data_dir = data_dir.split(\"/spawrious224/\")[\n","        0\n","    ]  # in case people pass in the wrong root_dir\n","    os.makedirs(data_dir, exist_ok=True)\n","    dataset_name = dataset_name.lower()\n","    if dataset_name.split(\"_\")[0] == \"m2m\":\n","        dataset_name = \"entire_dataset\"\n","    url_dict = {\n","        \"entire_dataset\": \"https://www.dropbox.com/s/hofkueo8qvaqlp3/spawrious224__entire_dataset.tar.gz?dl=1\",\n","        \"o2o_easy\": \"https://www.dropbox.com/s/kwhiv60ihxe3owy/spawrious224__o2o_easy.tar.gz?dl=1\",\n","        \"o2o_medium\": \"https://www.dropbox.com/s/x03gkhdwar5kht4/spawrious224__o2o_medium.tar.gz?dl=1\",\n","        \"o2o_hard\": \"https://www.dropbox.com/s/p1ry121m2gjj158/spawrious224__o2o_hard.tar.gz?dl=1\",\n","        # \"m2m\": \"https://www.dropbox.com/s/5usem63nfub266y/spawrious__m2m.tar.gz?dl=1\",\n","    }\n","    tar_file_name = f\"spawrious224__{dataset_name}.tar.gz\"\n","    tar_file_dst = os.path.join(data_dir, tar_file_name)\n","    url = url_dict[dataset_name]\n","\n","    # check if the dataset is already extracted\n","    if _check_images_availability(data_dir, dataset_name):\n","        print(\"Dataset already downloaded and extracted.\")\n","        return\n","    # check if the tar file is already downloaded\n","    else:\n","        if os.path.exists(tar_file_dst):\n","            print(\"Dataset already downloaded. Extracting...\")\n","            _extract_dataset_from_tar(\n","                tar_file_name, data_dir\n","            )\n","            return\n","        # download the tar file and extract from it\n","        else:\n","            print(\"Dataset not found. Downloading...\")\n","            response = urllib.request.urlopen(url)\n","            total_size = int(response.headers.get(\"Content-Length\", 0))\n","            block_size = 1024\n","            # Track progress of download\n","            progress_bar = tqdm(total=total_size, unit=\"iB\", unit_scale=True)\n","            with open(tar_file_dst, \"wb\") as f:\n","                while True:\n","                    buffer = response.read(block_size)\n","                    if not buffer:\n","                        break\n","                    f.write(buffer)\n","                    progress_bar.update(len(buffer))\n","            progress_bar.close()\n","            print(\"Dataset downloaded. Extracting...\")\n","            _extract_dataset_from_tar(\n","                tar_file_name, data_dir\n","            )\n","            return\n","\n","\n","class CustomImageFolder(Dataset):\n","    \"\"\"\n","    A class that takes one folder at a time and loads a set number of images in a folder and assigns them a specific class\n","    \"\"\"\n","\n","    def __init__(\n","        self, folder_path, class_index, location_index, limit=None, transform=None\n","    ):\n","        self.folder_path = folder_path\n","        self.class_index = class_index\n","        self.location_index = location_index\n","        self.image_paths = [\n","            os.path.join(folder_path, img)\n","            for img in os.listdir(folder_path)\n","            if img.endswith((\".png\", \".jpg\", \".jpeg\"))\n","        ]\n","        if limit:\n","            self.image_paths = self.image_paths[:limit]\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.image_paths)\n","\n","    def __getitem__(self, index: int) -> Tuple[Any, Any, Any]:\n","        img_path = self.image_paths[index]\n","        img = Image.open(img_path).convert(\"RGB\")\n","\n","        if self.transform:\n","            img = self.transform(img)\n","\n","        class_label = torch.tensor(self.class_index, dtype=torch.long)\n","        location_label = torch.tensor(self.location_index, dtype=torch.long)\n","        return img, class_label, location_label\n","\n","\n","class MultipleDomainDataset:\n","    N_STEPS = 5001  # Default, subclasses may override\n","    CHECKPOINT_FREQ = 100  # Default, subclasses may override\n","    N_WORKERS = 8  # Default, subclasses may override\n","    ENVIRONMENTS = None  # Subclasses should override\n","    INPUT_SHAPE = None  # Subclasses should override\n","\n","    def __getitem__(self, index):\n","        return self.datasets[index]\n","\n","    def __len__(self):\n","        return len(self.datasets)\n","\n","\n","def build_combination(benchmark_type, group, test, filler=None):\n","    total = 3168\n","    combinations = {}\n","    if \"m2m\" in benchmark_type:\n","        counts = [total, total]\n","        combinations[\"train_combinations\"] = {\n","            (\"bulldog\",): [(group[0], counts[0]), (group[1], counts[1])],\n","            (\"dachshund\",): [(group[1], counts[0]), (group[0], counts[1])],\n","            (\"labrador\",): [(group[2], counts[0]), (group[3], counts[1])],\n","            (\"corgi\",): [(group[3], counts[0]), (group[2], counts[1])],\n","        }\n","        combinations[\"test_combinations\"] = {\n","            (\"bulldog\",): [test[0], test[1]],\n","            (\"dachshund\",): [test[1], test[0]],\n","            (\"labrador\",): [test[2], test[3]],\n","            (\"corgi\",): [test[3], test[2]],\n","        }\n","    else:\n","        counts = [int(0.97 * total), int(0.87 * total)]\n","        combinations[\"train_combinations\"] = {\n","            (\"bulldog\",): [(group[0], counts[0]), (group[0], counts[1])],\n","            (\"dachshund\",): [(group[1], counts[0]), (group[1], counts[1])],\n","            (\"labrador\",): [(group[2], counts[0]), (group[2], counts[1])],\n","            (\"corgi\",): [(group[3], counts[0]), (group[3], counts[1])],\n","            (\"bulldog\", \"dachshund\", \"labrador\", \"corgi\"): [\n","                (filler, total - counts[0]),\n","                (filler, total - counts[1]),\n","            ],\n","        }\n","        combinations[\"test_combinations\"] = {\n","            (\"bulldog\",): [test[0], test[0]],\n","            (\"dachshund\",): [test[1], test[1]],\n","            (\"labrador\",): [test[2], test[2]],\n","            (\"corgi\",): [test[3], test[3]],\n","        }\n","    return combinations\n","\n","\n","def _get_combinations(benchmark_type: str) -> Tuple[dict, dict]:\n","    combinations = {\n","        \"o2o_easy\": (\n","            [\"desert\", \"jungle\", \"dirt\", \"snow\"],\n","            [\"dirt\", \"snow\", \"desert\", \"jungle\"],\n","            \"beach\",\n","        ),\n","        \"o2o_medium\": (\n","            [\"mountain\", \"beach\", \"dirt\", \"jungle\"],\n","            [\"jungle\", \"dirt\", \"beach\", \"snow\"],\n","            \"desert\",\n","        ),\n","        \"o2o_hard\": (\n","            [\"jungle\", \"mountain\", \"snow\", \"desert\"],\n","            [\"mountain\", \"snow\", \"desert\", \"jungle\"],\n","            \"beach\",\n","        ),\n","        \"m2m_hard\": (\n","            [\"dirt\", \"jungle\", \"snow\", \"beach\"],\n","            [\"snow\", \"beach\", \"dirt\", \"jungle\"],\n","            None,\n","        ),\n","        \"m2m_easy\": (\n","            [\"desert\", \"mountain\", \"dirt\", \"jungle\"],\n","            [\"dirt\", \"jungle\", \"mountain\", \"desert\"],\n","            None,\n","        ),\n","        \"m2m_medium\": (\n","            [\"beach\", \"snow\", \"mountain\", \"desert\"],\n","            [\"desert\", \"mountain\", \"beach\", \"snow\"],\n","            None,\n","        ),\n","    }\n","    if benchmark_type not in combinations:\n","        raise ValueError(\"Invalid benchmark type\")\n","    group, test, filler = combinations[benchmark_type]\n","    return build_combination(benchmark_type, group, test, filler)\n","\n","\n","class SpawriousBenchmark(MultipleDomainDataset):\n","    ENVIRONMENTS = [\"Test\", \"SC_group_1\", \"SC_group_2\"]\n","    input_shape = (3, 224, 224)\n","    num_classes = 4\n","    class_list = [\"bulldog\", \"corgi\", \"dachshund\", \"labrador\"]\n","    locations_list = [\"desert\", \"jungle\", \"dirt\", \"mountain\", \"snow\", \"beach\"]\n","\n","    def __init__(self, benchmark, root_dir, augment=True):\n","        combinations = _get_combinations(benchmark.lower())\n","        self.type1 = benchmark.lower().startswith(\"o2o\")\n","        train_datasets, test_datasets = self._prepare_data_lists(\n","            combinations[\"train_combinations\"],\n","            combinations[\"test_combinations\"],\n","            root_dir,\n","            augment,\n","        )\n","        self.datasets = [ConcatDataset(test_datasets)] + train_datasets\n","\n","    def get_train_dataset(self):\n","        return torch.utils.data.ConcatDataset(self.datasets[1:])\n","\n","    def get_test_dataset(self):\n","        return self.datasets[0]\n","\n","    # Prepares the train and test data lists by applying the necessary transformations.\n","    def _prepare_data_lists(\n","        self, train_combinations, test_combinations, root_dir, augment\n","    ):\n","        backbone = timm.create_model(\n","            # \"vit_so400m_patch14_siglip_384\",\n","            MODEL_NAME,\n","            pretrained=True,\n","            num_classes=0,\n","        ).eval()\n","        self.data_config = timm.data.resolve_model_data_config(backbone)\n","        test_transforms = timm.data.create_transform(\n","            **self.data_config, is_training=False\n","        )\n","\n","        # test_transforms = transforms.Compose(\n","        #     [\n","        #         transforms.Resize((self.input_shape[1], self.input_shape[2])),\n","        #         transforms.transforms.ToTensor(),\n","        #         transforms.Normalize(\n","        #             mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n","        #         ),\n","        #     ]\n","        # )\n","\n","        if augment:\n","            train_transforms = timm.data.create_transform(\n","                **self.data_config, is_training=True\n","            )\n","        else:\n","            train_transforms = test_transforms\n","        print(\"Creating Training Dataset:\")\n","        train_data_list = self._create_data_list(\n","            train_combinations, root_dir, train_transforms\n","        )\n","        print(\"Creating Testing Dataset:\")\n","        test_data_list = self._create_data_list(\n","            test_combinations, root_dir, test_transforms\n","        )\n","\n","        return train_data_list, test_data_list\n","\n","    # Creates a list of datasets based on the given combinations and transformations.\n","    def _create_data_list(self, combinations, root_dir, transforms):\n","        data_list = []\n","        if isinstance(combinations, dict):\n","            # Build class groups for a given set of combinations, root directory, and transformations.\n","            for_each_class_group = []\n","            cg_index = 0\n","            for classes, comb_list in combinations.items():\n","                for_each_class_group.append([])\n","                for ind, location_limit in enumerate(comb_list):\n","                    if isinstance(location_limit, tuple):\n","                        location, limit = location_limit\n","                    else:\n","                        location, limit = location_limit, None\n","                    cg_data_list = []\n","                    for cls in classes:\n","                        path = os.path.join(\n","                            root_dir,\n","                            \"spawrious224\",\n","                            f\"{0 if not self.type1 else ind}/{location}/{cls}\",\n","                        )\n","                        print(f\"    Combination: {location}/{cls}\")\n","                        print(f\"    Limit: {limit}\")\n","                        data = CustomImageFolder(\n","                            folder_path=path,\n","                            class_index=self.class_list.index(cls),\n","                            location_index=self.locations_list.index(location),\n","                            limit=limit,\n","                            transform=transforms,\n","                        )\n","                        cg_data_list.append(data)\n","\n","                    for_each_class_group[cg_index].append(ConcatDataset(cg_data_list))\n","                cg_index += 1\n","\n","            for group in range(len(for_each_class_group[0])):\n","                data_list.append(\n","                    ConcatDataset(\n","                        [\n","                            for_each_class_group[k][group]\n","                            for k in range(len(for_each_class_group))\n","                        ]\n","                    )\n","                )\n","        else:\n","            for location in combinations:\n","                path = os.path.join(root_dir, f\"{0}/{location}/\")\n","                data = ImageFolder(root=path, transform=transforms)\n","                data_list.append(data)\n","\n","        return data_list\n","\n","\n","def _check_images_availability(root_dir: str, dataset_type: str) -> bool:\n","    # Get the combinations for the given dataset type\n","    root_dir = root_dir.split(\"/spawrious224/\")[\n","        0\n","    ]  # in case people pass in the wrong root_dir\n","    if dataset_type == \"entire_dataset\":\n","        for dataset in [\"0\", \"1\", \"domain_adaptation_ds\"]:\n","            for location in [\"snow\", \"jungle\", \"desert\", \"dirt\", \"mountain\", \"beach\"]:\n","                for cls in [\"bulldog\", \"corgi\", \"dachshund\", \"labrador\"]:\n","                    path = os.path.join(\n","                        root_dir, \"spawrious224\", f\"{dataset}/{location}/{cls}\"\n","                    )\n","                    if not os.path.exists(path) or not any(\n","                        img.endswith((\".png\", \".jpg\", \".jpeg\"))\n","                        for img in os.listdir(path)\n","                    ):\n","                        return False\n","        return True\n","    combinations = _get_combinations(dataset_type.lower())\n","\n","    # Extract the train and test combinations\n","    train_combinations = combinations[\"train_combinations\"]\n","    test_combinations = combinations[\"test_combinations\"]\n","\n","    # Check if the relevant images for each combination are present in the root directory\n","    for combination in [train_combinations, test_combinations]:\n","        for classes, comb_list in combination.items():\n","            for ind, location_limit in enumerate(comb_list):\n","                if isinstance(location_limit, tuple):\n","                    location, limit = location_limit\n","                else:\n","                    location, limit = location_limit, None\n","\n","                for cls in classes:\n","                    path = os.path.join(\n","                        root_dir,\n","                        \"spawrious224\",\n","                        f\"{0 if not dataset_type.lower().startswith('o2o') else ind}/{location}/{cls}\",\n","                    )\n","\n","                    # If the path does not exist or there are no relevant images, return False\n","                    if not os.path.exists(path) or not any(\n","                        img.endswith((\".png\", \".jpg\", \".jpeg\"))\n","                        for img in os.listdir(path)\n","                    ):\n","                        return False\n","\n","    # If all the required images are present, return True\n","    return True\n","\n","\n","def get_spawrious_dataset(root_dir: str, dataset_name: str = \"entire_dataset\"):\n","    \"\"\"\n","    Returns the dataset as a torch dataset, and downloads dataset if dataset is not already available.\n","\n","    By default, the entire dataset is downloaded, which is necessary for m2m experiments, and domain adaptation experiments\n","    \"\"\"\n","    root_dir = root_dir.split(\"/spawrious224/\")[\n","        0\n","    ]  # in case people pass in the wrong root_dir\n","    assert dataset_name.lower() in {\n","        \"o2o_easy\",\n","        \"o2o_medium\",\n","        \"o2o_hard\",\n","        \"m2m_easy\",\n","        \"m2m_medium\",\n","        \"m2m_hard\",\n","        \"m2m\",\n","        \"entire_dataset\",\n","    }, f\"Invalid dataset type: {dataset_name}\"\n","    _download_dataset_if_not_available(dataset_name, root_dir)\n","    # TODO: get m2m to use entire dataset, not half of it\n","    return SpawriousBenchmark(dataset_name, root_dir, augment=True)"]},{"cell_type":"markdown","metadata":{"id":"n8I9YW0fc1Mk"},"source":["#1. Mount Drive, Define Directories for Saving Results"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16422,"status":"ok","timestamp":1731467284057,"user":{"displayName":"Thomas Garity","userId":"01070805114987455925"},"user_tz":300},"id":"BOvobZjydTf-","outputId":"9dec3444-3c67-465d-c04c-72802f65a52a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"Q8BFIyJRc6Po"},"source":["#2. Gather Dataset for Training and Testing\n","Using the SPAWRIOUS dataset at https://github.com/aengusl/spawrious/blob/main/example.py"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J5NdX3rZqiuJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1731467517279,"user_tz":300,"elapsed":122206,"user":{"displayName":"Thomas Garity","userId":"01070805114987455925"}},"outputId":"e5c90b92-cbb3-424e-e965-6d17adae3c62"},"outputs":[{"output_type":"stream","name":"stdout","text":["mkdir: cannot create directory ‘/content/data’: File exists\n"]}],"source":["project_dir = \"/content/drive/MyDrive/Classes/Fourth/Fall/CS2822R/CS2822_Final_Project\"\n","\n","drive_data_dir = os.path.join(project_dir, \"Datasets/spawrious224__o2o_easy.tar.gz\")\n","dataset_name = drive_data_dir.split('/')[-1]\n","\n","# Move Tar file from drive to local dir\n","!mkdir /content/data\n","!cp $drive_data_dir /content/data/"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RHYdywQiukTF"},"outputs":[],"source":["# Select a model and run the package's dataloader function\n","set_model_name(\"resnet18.a1_in1k\")"]},{"cell_type":"markdown","metadata":{"id":"fnYnPHYhdGxL"},"source":["#3. Helpers for Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ULeMgdQudxsR"},"outputs":[],"source":["# Training loop\n","def train(\n","    model: Module,\n","    train_loader: DataLoader,\n","    val_loader: DataLoader,\n","    optimizer: Optimizer,\n","    criterion: Module,\n","    num_epochs: int,\n","    device: torch.device,\n",") -> None:\n","    for epoch in tqdm(range(num_epochs), desc=\"Training. Epochs\", leave=False):\n","        running_loss = 0.0\n","        for inputs, labels, _ in tqdm(train_loader):  # third item is the location label\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            optimizer.zero_grad()\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","            running_loss += loss.item()\n","        print(\n","            f\"Epoch {epoch + 1}: Training Loss: {running_loss / len(train_loader):.3f}\"\n","        )\n","        print(\"Evaluating on validation set...\")\n","        val_acc = evaluate(model, val_loader, device)\n","        wandb.log(\n","            {\"train_loss\": running_loss / len(train_loader), \"val_acc\": val_acc},\n","            step=epoch,\n","        )\n","\n","# Eval loop\n","def evaluate(model: Module, loader: DataLoader, device: torch.device) -> float:\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for inputs, labels, _ in tqdm(\n","            loader, desc=\"Evaluating\", leave=False\n","        ):  # third item is the location label\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            outputs = model(inputs)\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","    acc = 100 * correct / total\n","    print(f\"Acc: {acc:.3f}%\")\n","    return acc\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"haOz4LuodJ5D"},"source":["#4. Initialize Model, Run Training"]},{"cell_type":"markdown","metadata":{"id":"w20O7xeTd3VS"},"source":["Helpers for loading and initializing model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v_EsRa9ud5yV"},"outputs":[],"source":["# Class to modify model\n","class ClassifierOnTop(nn.Module):\n","    def __init__(self, num_classes):\n","        super().__init__()\n","        self.backbone = timm.create_model(\n","            # \"vit_so400m_patch14_siglip_384\",\n","            MODEL_NAME,\n","            pretrained=True,\n","            num_classes=0,\n","        ).eval()\n","        self.linear = nn.Linear(1152, num_classes)\n","        if MODEL_NAME == 'swin_base_patch4_window7_224.ms_in22k_ft_in1k':\n","            self.linear = nn.Linear(1024, num_classes)\n","        elif MODEL_NAME == 'deit3_base_patch16_224.fb_in22k_ft_in1k':\n","            self.linear = nn.Linear(768, num_classes)\n","        elif MODEL_NAME == 'beit_base_patch16_224.in22k_ft_in22k_in1k':\n","            self.linear = nn.Linear(768, num_classes)\n","        elif MODEL_NAME == 'eva02_base_patch14_448.mim_in22k_ft_in22k_in1k':\n","            self.linear = nn.Linear(768, num_classes)\n","        elif MODEL_NAME == 'levit_128s.fb_dist_in1k':\n","            self.linear = nn.Linear(384, num_classes)\n","\n","    def forward(self, x):\n","        with torch.no_grad():\n","            x = self.backbone(x)\n","        return self.linear(x)\n","\n","\n","def get_model(args: argparse.Namespace) -> Module:\n","    if args.model == \"siglip\":\n","        model = ClassifierOnTop(num_classes=4)\n","    else:\n","        model = models.resnet18(pretrained=True)\n","        model.fc = torch.nn.Linear(512, 4)\n","    return model\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JCewXTM7y0e2"},"outputs":[],"source":["def main(dataset) -> None:\n","    args = Args(model=\"Resnet\")\n","    experiment_name = f\"{dataset}_{MODEL_NAME.split('_')[0]}-e={args.num_epochs}-lr={args.lr}\"\n","    experiment_name = f\"{experiment_name}_limit=20\"\n","\n","    wandb.init(project=\"spawrious\", name=experiment_name, config=args)\n","    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","    spawrious = get_spawrious_dataset(dataset_name=args.dataset, root_dir=args.data_dir)\n","    train_set = spawrious.get_train_dataset()\n","    test_set = spawrious.get_test_dataset()\n","    val_size = int(len(train_set) * args.val_split)\n","    train_set, val_set = torch.utils.data.random_split(\n","        train_set, [len(train_set) - val_size, val_size]\n","    )\n","    train_loader = torch.utils.data.DataLoader(\n","        train_set,\n","        batch_size=args.batch_size,\n","        shuffle=True,\n","        num_workers=args.num_workers,\n","    )\n","    val_loader = torch.utils.data.DataLoader(\n","        val_set, batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers\n","    )\n","    test_loader = torch.utils.data.DataLoader(\n","        test_set,\n","        batch_size=args.batch_size,\n","        shuffle=False,\n","        num_workers=args.num_workers,\n","    )\n","\n","    model = get_model(args)\n","    model.to(device)\n","\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.AdamW(model.parameters(), lr=args.lr)\n","    train(\n","        model,\n","        train_loader,\n","        val_loader,\n","        optimizer,\n","        criterion,\n","        args.num_epochs,\n","        device,\n","    )\n","    print(\"Finished training, now evaluating on test set.\")\n","    torch.save(model.state_dict(), f\"{experiment_name}.pt\")\n","    test_acc = evaluate(model, test_loader, device)\n","    wandb.log({\"final_test_acc\": test_acc}, step=args.num_epochs)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p8364coUz_Eh"},"outputs":[],"source":["# Artificially create args\n","class Args:\n","    def __init__(self, model=\"siglip\",\n","                 dataset=\"o2o_easy\", data_dir='/content/data/',\n","                 num_epochs=2, val_split=0.1,\n","                 batch_size=128,num_workers=2, lr=0.01, momentum=0.9):\n","        self.model = model\n","        self.dataset = dataset\n","        self.data_dir = data_dir\n","        self.num_epochs = num_epochs\n","        self.val_split = val_split\n","        self.batch_size = batch_size\n","        self.num_workers = num_workers\n","        self.lr = lr\n","        self.momentum = momentum\n","\n","args = Args(model=\"Resnet\", num_epochs=10)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["a6a46eec11334a558422a55eac79681d","aa1078b198f74388be00dfdacd6c5d36","30f7c243d141412f9d6d7098a13b6923","cfbe7f2d8edf4b55a9aaf106fcdb8fd3","a3c41fbce4f44fd08009760e0438f3f8","85ddf47631d342b79c934b3d2129a0f3","af3b72c353ce423bbfefd40cc1155a7a","0d29b45a6cfe4b6398625e23a4d1541c","ca04149ba200417988f6d97520e81db0","ceed3e157a1846939456ad52b1fec58c","cbc695adb7254ac2b6a25134921800b9"]},"id":"zffINaTleNdb","outputId":"b0705f3d-eda4-4dc4-fe38-611950f90a7e","executionInfo":{"status":"error","timestamp":1731468284595,"user_tz":300,"elapsed":728557,"user":{"displayName":"Thomas Garity","userId":"01070805114987455925"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","        window._wandbApiKey = new Promise((resolve, reject) => {\n","            function loadScript(url) {\n","            return new Promise(function(resolve, reject) {\n","                let newScript = document.createElement(\"script\");\n","                newScript.onerror = reject;\n","                newScript.onload = resolve;\n","                document.body.appendChild(newScript);\n","                newScript.src = url;\n","            });\n","            }\n","            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n","            const iframe = document.createElement('iframe')\n","            iframe.style.cssText = \"width:0;height:0;border:none\"\n","            document.body.appendChild(iframe)\n","            const handshake = new Postmate({\n","                container: iframe,\n","                url: 'https://wandb.ai/authorize'\n","            });\n","            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n","            handshake.then(function(child) {\n","                child.on('authorize', data => {\n","                    clearTimeout(timeout)\n","                    resolve(data)\n","                });\n","            });\n","            })\n","        });\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"]},{"name":"stdout","output_type":"stream","text":[" ··········\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.18.6"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20241113_031249-pqqfvo6j</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href='https://wandb.ai/tgarity-harvard-university/spawrious/runs/pqqfvo6j' target=\"_blank\">o2o_easy_resnet18.a1-e=2-lr=0.01_limit=20</a></strong> to <a href='https://wandb.ai/tgarity-harvard-university/spawrious' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View project at <a href='https://wandb.ai/tgarity-harvard-university/spawrious' target=\"_blank\">https://wandb.ai/tgarity-harvard-university/spawrious</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run at <a href='https://wandb.ai/tgarity-harvard-university/spawrious/runs/pqqfvo6j' target=\"_blank\">https://wandb.ai/tgarity-harvard-university/spawrious/runs/pqqfvo6j</a>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Dataset already downloaded. Extracting...\n","Extracting dataset...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/46.8M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a6a46eec11334a558422a55eac79681d"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Creating Training Dataset:\n","    Combination: desert/bulldog\n","    Limit: 3072\n","    Combination: desert/bulldog\n","    Limit: 2756\n","    Combination: jungle/dachshund\n","    Limit: 3072\n","    Combination: jungle/dachshund\n","    Limit: 2756\n","    Combination: dirt/labrador\n","    Limit: 3072\n","    Combination: dirt/labrador\n","    Limit: 2756\n","    Combination: snow/corgi\n","    Limit: 3072\n","    Combination: snow/corgi\n","    Limit: 2756\n","    Combination: beach/bulldog\n","    Limit: 96\n","    Combination: beach/dachshund\n","    Limit: 96\n","    Combination: beach/labrador\n","    Limit: 96\n","    Combination: beach/corgi\n","    Limit: 96\n","    Combination: beach/bulldog\n","    Limit: 412\n","    Combination: beach/dachshund\n","    Limit: 412\n","    Combination: beach/labrador\n","    Limit: 412\n","    Combination: beach/corgi\n","    Limit: 412\n","Creating Testing Dataset:\n","    Combination: dirt/bulldog\n","    Limit: None\n","    Combination: dirt/bulldog\n","    Limit: None\n","    Combination: snow/dachshund\n","    Limit: None\n","    Combination: snow/dachshund\n","    Limit: None\n","    Combination: desert/labrador\n","    Limit: None\n","    Combination: desert/labrador\n","    Limit: None\n","    Combination: jungle/corgi\n","    Limit: None\n","    Combination: jungle/corgi\n","    Limit: None\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n","100%|██████████| 44.7M/44.7M [00:00<00:00, 92.6MB/s]\n","Training. Epochs:   0%|          | 0/2 [00:00<?, ?it/s]\n","  0%|          | 0/179 [00:00<?, ?it/s]\u001b[A\n","  1%|          | 1/179 [00:54<2:40:36, 54.14s/it]\u001b[A\n","  1%|          | 2/179 [01:37<2:21:27, 47.95s/it]\u001b[A\n","  2%|▏         | 3/179 [02:21<2:14:24, 45.82s/it]\u001b[A\n","  2%|▏         | 4/179 [03:03<2:09:27, 44.39s/it]\u001b[A\n","  3%|▎         | 5/179 [03:43<2:04:45, 43.02s/it]\u001b[A\n","  3%|▎         | 6/179 [04:31<2:08:44, 44.65s/it]\u001b[A\n","  4%|▍         | 7/179 [05:19<2:10:34, 45.55s/it]\u001b[A\n","  4%|▍         | 8/179 [06:04<2:09:53, 45.58s/it]\u001b[A\n","  5%|▌         | 9/179 [06:49<2:08:13, 45.25s/it]\u001b[A\n","  6%|▌         | 10/179 [07:32<2:05:19, 44.49s/it]\u001b[A\n","  6%|▌         | 11/179 [08:17<2:05:11, 44.71s/it]\u001b[A\n","  7%|▋         | 12/179 [09:51<2:17:05, 49.26s/it]\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-ddd7bdcafd28>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mset_model_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-11-af1fc2fe61f0>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdamW\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     train(\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-9-62e52cf48f4a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, val_loader, optimizer, criterion, num_epochs, device)\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             )\n\u001b[0;32m--> 581\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    826\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# Run Training\n","if __name__ == \"__main__\":\n","    dataset_choices = [\n","            \"o2o_easy\",\n","            #\"o2o_medium\",\n","            #\"o2o_hard\",\n","            #\"m2m_easy\",\n","            #\"m2m_medium\",\n","            #\"m2m_hard\",\n","        ]\n","    MODEL_NAME = 'resnet18.a1_in1k'\n","    # MODEL_NAME = \"vit_so400m_patch14_siglip_384\"\n","    # MODEL_NAME = 'swin_base_patch4_window7_224.ms_in22k_ft_in1k'\n","    # MODEL_NAME = 'deit3_base_patch16_224.fb_in22k_ft_in1k'\n","    # MODEL_NAME = 'beit_base_patch16_224.in22k_ft_in22k_in1k'\n","    # MODEL_NAME = 'eva02_base_patch14_448.mim_in22k_ft_in22k_in1k'\n","    # MODEL_NAME = 'levit_128s.fb_dist_in1k'\n","    model_name_choices = [\n","        'resnet18.a1_in1k',\n","        #'vit_so400m_patch14_siglip_384',\n","        #'swin_base_patch4_window7_224.ms_in22k_ft_in1k',\n","        #'deit3_base_patch16_224.fb_in22k_ft_in1k',\n","        #'beit_base_patch16_224.in22k_ft_in22k_in1k',\n","        # 'eva02_base_patch14_448.mim_in22k_ft_in22k_in1k',\n","        #'levit_128s.fb_dist_in1k',\n","    ]\n","\n","    # Train just our model\n","\n","    # If you want to run training on all model architectures\n","    for dataset in dataset_choices:\n","        for model_name in model_name_choices:\n","\n","            MODEL_NAME=model_name\n","            set_model_name(MODEL_NAME)\n","\n","            main(dataset)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"067t86OzdMt1"},"source":["#5. Examine Training Results"]},{"cell_type":"markdown","metadata":{"id":"DPdEImz4wph8"},"source":["Next:\n","1. Create a folder of the CORGI class, sampling from all regions\n","2. Run CRAFT on the Corgi\n","3. Denote which concepts are (i) True to subject or (ii) Spurious Correlates\n","4. Determine a way to know if Spurious correlates are an issue -- likely **if a prediction can be made with solely spurious correlates, then we know that there is something wrong with the model**\n","    - We can do this by:\n","        - Find the concept that is the spurious correlate\n","        - Compare importance across the class to the other ones\n","        - find images with high importance and low importance of that correlate\n","        - measure difference in classification\n","        - So something like (class_includes_correlate - class_excludes_correlate) pairwise in the dataset"]},{"cell_type":"code","source":["import glob\n","\n","# Grab random images for ACE\n","dir = \"/content/data/spawrious224/0\"\n","images = glob.glob(f\"{dir}/***/**/*.png\", recursive=True)\n","\n","!mkdir /content/data/random_images\n","for i in range(50):\n","    !mkdir /content/data/random_images/\n","    for i in range(100):\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JoRQsQRDjWo1","executionInfo":{"status":"ok","timestamp":1731468456419,"user_tz":300,"elapsed":286,"user":{"displayName":"Thomas Garity","userId":"01070805114987455925"}},"outputId":"5db312f1-f779-434e-8c9c-713317ca3225"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["38016\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"NEHwOpY-kAYV"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EkHmxqWg9HPo"},"outputs":[],"source":["# Move models to model drive\n","model_dir = os.path.join(project_dir, \"Models\")\n","model = '/content/o2o_medium_resnet18.a1-e=2-lr=0.01_limit=20.pt'\n","!cp $model $model_dir"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"a6a46eec11334a558422a55eac79681d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_aa1078b198f74388be00dfdacd6c5d36","IPY_MODEL_30f7c243d141412f9d6d7098a13b6923","IPY_MODEL_cfbe7f2d8edf4b55a9aaf106fcdb8fd3"],"layout":"IPY_MODEL_a3c41fbce4f44fd08009760e0438f3f8"}},"aa1078b198f74388be00dfdacd6c5d36":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_85ddf47631d342b79c934b3d2129a0f3","placeholder":"​","style":"IPY_MODEL_af3b72c353ce423bbfefd40cc1155a7a","value":"model.safetensors: 100%"}},"30f7c243d141412f9d6d7098a13b6923":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0d29b45a6cfe4b6398625e23a4d1541c","max":46807446,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ca04149ba200417988f6d97520e81db0","value":46807446}},"cfbe7f2d8edf4b55a9aaf106fcdb8fd3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ceed3e157a1846939456ad52b1fec58c","placeholder":"​","style":"IPY_MODEL_cbc695adb7254ac2b6a25134921800b9","value":" 46.8M/46.8M [00:00&lt;00:00, 90.0MB/s]"}},"a3c41fbce4f44fd08009760e0438f3f8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"85ddf47631d342b79c934b3d2129a0f3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"af3b72c353ce423bbfefd40cc1155a7a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0d29b45a6cfe4b6398625e23a4d1541c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ca04149ba200417988f6d97520e81db0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ceed3e157a1846939456ad52b1fec58c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cbc695adb7254ac2b6a25134921800b9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}